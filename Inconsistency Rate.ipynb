{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQ3XbUbRPjq2djVi/x+DhH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Feranie/Hierarchical-Classification-Project/blob/main/Inconsistency%20Rate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZIMGw-w8suR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "from collections import defaultdict, Counter\n",
        "from typing import List, Tuple, Dict, Any\n",
        "\n",
        "# --- ARFF File Reading ---\n",
        "def read_arff_file(file_path):\n",
        "    \"\"\"\n",
        "    Read ARFF file and return pandas DataFrame.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    attributes = []\n",
        "    current_section = None\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if not line or line.startswith('%'):\n",
        "                continue\n",
        "\n",
        "            if '@relation' in line.lower():\n",
        "                current_section = 'relation'\n",
        "            elif '@attribute' in line.lower():\n",
        "                current_section = 'attribute'\n",
        "                match = re.match(r'@attribute\\s+([^\\s]+)\\s+.*', line, re.IGNORECASE)\n",
        "                if match:\n",
        "                    attributes.append(match.group(1))\n",
        "            elif '@data' in line.lower():\n",
        "                current_section = 'data'\n",
        "            elif current_section == 'data':\n",
        "                values = re.findall(r'[^,]+(?:,(?=[^,]$))?', line)\n",
        "                values = [v.strip('\" ') for v in values]\n",
        "                if len(values) == len(attributes):\n",
        "                    data.append(values)\n",
        "\n",
        "    return pd.DataFrame(data, columns=attributes)\n",
        "\n",
        "# --- ARFF File Saving ---\n",
        "def save_to_arff(df, file_path, relation_name=\"filtered_data\"):\n",
        "    \"\"\"\n",
        "    Save DataFrame to ARFF file format.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"@relation {relation_name}\\n\\n\")\n",
        "\n",
        "        for column in df.columns:\n",
        "            unique_values = df[column].unique()\n",
        "            if all(isinstance(val, str) for val in unique_values):\n",
        "                vals = ','.join(sorted(set(unique_values)))\n",
        "                f.write(f\"@attribute {column} {{{vals}}}\\n\")\n",
        "            else:\n",
        "                f.write(f\"@attribute {column} numeric\\n\")\n",
        "\n",
        "        f.write(\"\\n@data\\n\")\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(\",\".join(map(str, row)) + \"\\n\")\n",
        "\n",
        "class ConsistencyMeasure:\n",
        "    \"\"\"\n",
        "    Implementation of the consistency measure (inconsistency rate) as described in:\n",
        "    M. Dash, H. Liu / Artificial Intelligence 151 (2003) 155–176\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data: pd.DataFrame, class_column: str):\n",
        "        \"\"\"\n",
        "        Initialize the consistency measure calculator.\n",
        "\n",
        "        Args:\n",
        "            data: DataFrame containing the dataset\n",
        "            class_column: Name of the class/target column\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.class_column = class_column\n",
        "        self.feature_columns = [col for col in data.columns if col != class_column]\n",
        "        self.total_instances = len(data)\n",
        "\n",
        "    def calculate_inconsistency_rate(self, feature_subset: List[str]) -> float:\n",
        "        \"\"\"\n",
        "        Calculate the inconsistency rate for a given feature subset.\n",
        "\n",
        "        Args:\n",
        "            feature_subset: List of feature names to consider\n",
        "\n",
        "        Returns:\n",
        "            Inconsistency rate (IR) for the feature subset\n",
        "        \"\"\"\n",
        "        if not feature_subset:\n",
        "            return 0.0\n",
        "\n",
        "        # Validate feature subset\n",
        "        invalid_features = [f for f in feature_subset if f not in self.feature_columns]\n",
        "        if invalid_features:\n",
        "            raise ValueError(f\"Invalid features: {invalid_features}\")\n",
        "\n",
        "        # Group instances by pattern (combination of feature values)\n",
        "        patterns = self._group_by_pattern(feature_subset)\n",
        "\n",
        "        # Calculate inconsistency count for each pattern\n",
        "        total_inconsistency_count = 0\n",
        "\n",
        "        for pattern, instances in patterns.items():\n",
        "            inconsistency_count = self._calculate_pattern_inconsistency(instances)\n",
        "            total_inconsistency_count += inconsistency_count\n",
        "\n",
        "        # Calculate inconsistency rate\n",
        "        inconsistency_rate = total_inconsistency_count / self.total_instances\n",
        "        return inconsistency_rate\n",
        "\n",
        "    def _group_by_pattern(self, feature_subset: List[str]) -> Dict[Tuple, List[Any]]:\n",
        "        \"\"\"\n",
        "        Group instances by their pattern (feature value combinations).\n",
        "\n",
        "        Args:\n",
        "            feature_subset: List of feature names\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping patterns to lists of class labels\n",
        "        \"\"\"\n",
        "        patterns = defaultdict(list)\n",
        "\n",
        "        for _, row in self.data.iterrows():\n",
        "            # Create pattern tuple from feature values\n",
        "            pattern = tuple(row[feature] for feature in feature_subset)\n",
        "            class_label = row[self.class_column]\n",
        "            patterns[pattern].append(class_label)\n",
        "\n",
        "        return patterns\n",
        "\n",
        "    def _calculate_pattern_inconsistency(self, class_labels: List[Any]) -> int:\n",
        "        \"\"\"\n",
        "        Calculate inconsistency count for a single pattern.\n",
        "\n",
        "        Args:\n",
        "            class_labels: List of class labels for instances with the same pattern\n",
        "\n",
        "        Returns:\n",
        "            Inconsistency count for this pattern\n",
        "        \"\"\"\n",
        "        if len(class_labels) <= 1:\n",
        "            return 0\n",
        "\n",
        "        # Count occurrences of each class label\n",
        "        label_counts = Counter(class_labels)\n",
        "\n",
        "        # Find the maximum count (most frequent class)\n",
        "        max_count = max(label_counts.values())\n",
        "\n",
        "        # Inconsistency count = total instances - max count\n",
        "        inconsistency_count = len(class_labels) - max_count\n",
        "\n",
        "        return inconsistency_count\n",
        "\n",
        "    def analyze_pattern_details(self, feature_subset: List[str]) -> Dict:\n",
        "        \"\"\"\n",
        "        Provide detailed analysis of patterns and their inconsistencies.\n",
        "\n",
        "        Args:\n",
        "            feature_subset: List of feature names to analyze\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with detailed pattern analysis\n",
        "        \"\"\"\n",
        "        patterns = self._group_by_pattern(feature_subset)\n",
        "        analysis = {\n",
        "            'total_patterns': len(patterns),\n",
        "            'inconsistent_patterns': 0,\n",
        "            'pattern_details': [],\n",
        "            'total_inconsistency_count': 0\n",
        "        }\n",
        "\n",
        "        for pattern, class_labels in patterns.items():\n",
        "            label_counts = Counter(class_labels)\n",
        "            inconsistency_count = self._calculate_pattern_inconsistency(class_labels)\n",
        "\n",
        "            pattern_info = {\n",
        "                'pattern': pattern,\n",
        "                'total_instances': len(class_labels),\n",
        "                'class_distribution': dict(label_counts),\n",
        "                'inconsistency_count': inconsistency_count,\n",
        "                'is_inconsistent': inconsistency_count > 0\n",
        "            }\n",
        "\n",
        "            analysis['pattern_details'].append(pattern_info)\n",
        "            analysis['total_inconsistency_count'] += inconsistency_count\n",
        "\n",
        "            if inconsistency_count > 0:\n",
        "                analysis['inconsistent_patterns'] += 1\n",
        "\n",
        "        analysis['inconsistency_rate'] = analysis['total_inconsistency_count'] / self.total_instances\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    def compare_feature_subsets(self, feature_subsets: List[List[str]]) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Compare inconsistency rates across multiple feature subsets.\n",
        "\n",
        "        Args:\n",
        "            feature_subsets: List of feature subset lists to compare\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with comparison results\n",
        "        \"\"\"\n",
        "        results = []\n",
        "\n",
        "        for i, subset in enumerate(feature_subsets):\n",
        "            ir = self.calculate_inconsistency_rate(subset)\n",
        "            results.append({\n",
        "                'subset_id': i,\n",
        "                'features': subset,\n",
        "                'num_features': len(subset),\n",
        "                'inconsistency_rate': ir,\n",
        "                'consistency_score': 1 - ir  # Higher is better\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(results).sort_values('inconsistency_rate')\n",
        "\n",
        "class RandomRestartHillClimbing:\n",
        "    \"\"\"\n",
        "    Random Restart Hill Climbing algorithm to find optimal feature subset\n",
        "    that minimizes inconsistency rate (IRH).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, consistency_measure: ConsistencyMeasure, max_restarts=10,\n",
        "                 max_iterations=100, min_features=1):\n",
        "        \"\"\"\n",
        "        Initialize RRHC optimizer.\n",
        "\n",
        "        Args:\n",
        "            consistency_measure: ConsistencyMeasure instance\n",
        "            max_restarts: Maximum number of random restarts\n",
        "            max_iterations: Maximum iterations per restart\n",
        "            min_features: Minimum number of features to keep\n",
        "        \"\"\"\n",
        "        self.cm = consistency_measure\n",
        "        self.max_restarts = max_restarts\n",
        "        self.max_iterations = max_iterations\n",
        "        self.min_features = min_features\n",
        "        self.all_features = self.cm.feature_columns.copy()\n",
        "\n",
        "    def optimize(self, verbose=True):\n",
        "        \"\"\"\n",
        "        Run Random Restart Hill Climbing to find optimal feature subset.\n",
        "\n",
        "        Args:\n",
        "            verbose: Print progress information\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with optimization results\n",
        "        \"\"\"\n",
        "        best_subset = None\n",
        "        best_ir = float('inf')\n",
        "        best_restart = -1\n",
        "        all_results = []\n",
        "\n",
        "        for restart in range(self.max_restarts):\n",
        "            if verbose:\n",
        "                print(f\"\\n--- Restart {restart + 1}/{self.max_restarts} ---\")\n",
        "\n",
        "            # Generate random initial subset\n",
        "            initial_size = random.randint(self.min_features, len(self.all_features))\n",
        "            current_subset = random.sample(self.all_features, initial_size)\n",
        "            current_ir = self.cm.calculate_inconsistency_rate(current_subset)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Initial subset: {len(current_subset)} features, IR: {current_ir:.4f}\")\n",
        "\n",
        "            # Hill climbing from this starting point\n",
        "            result = self._hill_climb(current_subset, current_ir, verbose)\n",
        "            all_results.append(result)\n",
        "\n",
        "            # Update global best\n",
        "            if result['final_ir'] < best_ir:\n",
        "                best_ir = result['final_ir']\n",
        "                best_subset = result['final_subset'].copy()\n",
        "                best_restart = restart\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Final: {len(result['final_subset'])} features, IR: {result['final_ir']:.4f}\")\n",
        "\n",
        "        return {\n",
        "            'best_subset': best_subset,\n",
        "            'best_ir': best_ir,\n",
        "            'best_restart': best_restart,\n",
        "            'all_results': all_results,\n",
        "            'total_evaluations': sum(r['evaluations'] for r in all_results)\n",
        "        }\n",
        "\n",
        "    def _hill_climb(self, initial_subset, initial_ir, verbose=False):\n",
        "        \"\"\"\n",
        "        Perform hill climbing from an initial subset.\n",
        "        \"\"\"\n",
        "        current_subset = initial_subset.copy()\n",
        "        current_ir = initial_ir\n",
        "        iteration = 0\n",
        "        evaluations = 1  # Count initial evaluation\n",
        "        improvements = 0\n",
        "\n",
        "        while iteration < self.max_iterations:\n",
        "            iteration += 1\n",
        "            improved = False\n",
        "\n",
        "            # Generate all neighbors (add/remove one feature)\n",
        "            neighbors = self._generate_neighbors(current_subset)\n",
        "\n",
        "            # Evaluate all neighbors\n",
        "            for neighbor in neighbors:\n",
        "                if len(neighbor) < self.min_features:\n",
        "                    continue\n",
        "\n",
        "                neighbor_ir = self.cm.calculate_inconsistency_rate(neighbor)\n",
        "                evaluations += 1\n",
        "\n",
        "                # Accept if better (lower IR)\n",
        "                if neighbor_ir < current_ir:\n",
        "                    current_subset = neighbor.copy()\n",
        "                    current_ir = neighbor_ir\n",
        "                    improved = True\n",
        "                    improvements += 1\n",
        "\n",
        "                    if verbose and iteration % 10 == 0:\n",
        "                        print(f\"  Iteration {iteration}: {len(current_subset)} features, IR: {current_ir:.4f}\")\n",
        "                    break\n",
        "\n",
        "            if not improved:\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'final_subset': current_subset,\n",
        "            'final_ir': current_ir,\n",
        "            'iterations': iteration,\n",
        "            'evaluations': evaluations,\n",
        "            'improvements': improvements\n",
        "        }\n",
        "\n",
        "    def _generate_neighbors(self, current_subset):\n",
        "        \"\"\"\n",
        "        Generate neighbor subsets by adding/removing one feature.\n",
        "        \"\"\"\n",
        "        neighbors = []\n",
        "        current_set = set(current_subset)\n",
        "\n",
        "        # Remove one feature (if possible)\n",
        "        if len(current_subset) > self.min_features:\n",
        "            for feature in current_subset:\n",
        "                neighbor = [f for f in current_subset if f != feature]\n",
        "                neighbors.append(neighbor)\n",
        "\n",
        "        # Add one feature\n",
        "        available_features = [f for f in self.all_features if f not in current_set]\n",
        "        for feature in available_features:\n",
        "            neighbor = current_subset + [feature]\n",
        "            neighbors.append(neighbor)\n",
        "\n",
        "        return neighbors\n",
        "\n",
        "def generate_feature_subsets(feature_columns, max_subset_size=None, include_all_sizes=True):\n",
        "    \"\"\"\n",
        "    Generate feature subsets automatically from the dataset columns.\n",
        "\n",
        "    Args:\n",
        "        feature_columns: List of feature column names from the dataset\n",
        "        max_subset_size: Maximum size of subsets to generate (default: all features)\n",
        "        include_all_sizes: Whether to include subsets of all sizes from 1 to max_subset_size\n",
        "\n",
        "    Returns:\n",
        "        List of feature subsets\n",
        "    \"\"\"\n",
        "    from itertools import combinations\n",
        "\n",
        "    if max_subset_size is None:\n",
        "        max_subset_size = len(feature_columns)\n",
        "\n",
        "    feature_subsets = []\n",
        "\n",
        "    if include_all_sizes:\n",
        "        # Generate subsets of all sizes from 1 to max_subset_size\n",
        "        for size in range(1, min(max_subset_size + 1, len(feature_columns) + 1)):\n",
        "            for subset in combinations(feature_columns, size):\n",
        "                feature_subsets.append(list(subset))\n",
        "    else:\n",
        "        # Generate only subsets of max_subset_size\n",
        "        for subset in combinations(feature_columns, max_subset_size):\n",
        "            feature_subsets.append(list(subset))\n",
        "\n",
        "    return feature_subsets\n",
        "\n",
        "def generate_smart_feature_subsets(feature_columns, max_combinations=200, include_large_subsets=True):\n",
        "    \"\"\"\n",
        "    Generate a smart selection of feature subsets for large datasets.\n",
        "    Uses strategic sampling to avoid combinatorial explosion.\n",
        "\n",
        "    Args:\n",
        "        feature_columns: List of feature column names\n",
        "        max_combinations: Maximum number of subsets to generate\n",
        "        include_large_subsets: Whether to include some larger subsets\n",
        "\n",
        "    Returns:\n",
        "        List of feature subsets\n",
        "    \"\"\"\n",
        "    from itertools import combinations\n",
        "    import random\n",
        "\n",
        "    feature_subsets = []\n",
        "    n_features = len(feature_columns)\n",
        "\n",
        "    print(f\"Génération intelligente de sous-ensembles pour {n_features} attributs...\")\n",
        "\n",
        "    # Always include individual features\n",
        "    print(\"Ajout des attributs individuels...\")\n",
        "    for feature in feature_columns:\n",
        "        feature_subsets.append([feature])\n",
        "\n",
        "    remaining_budget = max_combinations - len(feature_subsets)\n",
        "\n",
        "    # Add pairs strategically\n",
        "    if n_features <= 20:\n",
        "        # Small enough to include all pairs\n",
        "        print(\"Ajout de toutes les paires...\")\n",
        "        for pair in combinations(feature_columns, 2):\n",
        "            feature_subsets.append(list(pair))\n",
        "            remaining_budget -= 1\n",
        "            if remaining_budget <= 0:\n",
        "                break\n",
        "    else:\n",
        "        # Sample pairs\n",
        "        pair_budget = min(100, remaining_budget // 2)\n",
        "        print(f\"Échantillonnage de {pair_budget} paires...\")\n",
        "        all_pairs = list(combinations(feature_columns, 2))\n",
        "        sampled_pairs = random.sample(all_pairs, min(pair_budget, len(all_pairs)))\n",
        "        for pair in sampled_pairs:\n",
        "            feature_subsets.append(list(pair))\n",
        "        remaining_budget -= len(sampled_pairs)\n",
        "\n",
        "    # Add some larger subsets if requested and budget allows\n",
        "    if include_large_subsets and remaining_budget > 0:\n",
        "        for size in [3, 4, 5, 10, 15, 20]:\n",
        "            if size > n_features or remaining_budget <= 0:\n",
        "                break\n",
        "\n",
        "            size_budget = min(20, remaining_budget // (6 - (size - 3)),\n",
        "                            remaining_budget if size >= 10 else remaining_budget)\n",
        "\n",
        "            if size_budget > 0:\n",
        "                print(f\"Échantillonnage de {size_budget} sous-ensembles de taille {size}...\")\n",
        "                all_combinations = list(combinations(feature_columns, size))\n",
        "                sample_size = min(size_budget, len(all_combinations))\n",
        "                sampled_combinations = random.sample(all_combinations, sample_size)\n",
        "\n",
        "                for combo in sampled_combinations:\n",
        "                    feature_subsets.append(list(combo))\n",
        "\n",
        "                remaining_budget -= sample_size\n",
        "\n",
        "    # Add some random subsets of various sizes\n",
        "    if remaining_budget > 0:\n",
        "        print(f\"Ajout de {remaining_budget} sous-ensembles aléatoires...\")\n",
        "        for _ in range(remaining_budget):\n",
        "            size = random.randint(2, min(25, n_features))\n",
        "            random_subset = random.sample(feature_columns, size)\n",
        "            feature_subsets.append(random_subset)\n",
        "\n",
        "    print(f\"Total généré: {len(feature_subsets)} sous-ensembles\")\n",
        "    return feature_subsets\n",
        "\n",
        "def evaluate_subsets_with_progress(cm, feature_subsets, max_time_minutes=30):\n",
        "    \"\"\"\n",
        "    Evaluate feature subsets with progress tracking and time limit.\n",
        "\n",
        "    Args:\n",
        "        cm: ConsistencyMeasure instance\n",
        "        feature_subsets: List of feature subsets to evaluate\n",
        "        max_time_minutes: Maximum time to spend on evaluation\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with results\n",
        "    \"\"\"\n",
        "    import time\n",
        "\n",
        "    results = []\n",
        "    start_time = time.time()\n",
        "    max_time_seconds = max_time_minutes * 60\n",
        "\n",
        "    print(f\"\\nÉvaluation de {len(feature_subsets)} sous-ensembles...\")\n",
        "    print(f\"Temps maximum alloué: {max_time_minutes} minutes\")\n",
        "\n",
        "    for i, subset in enumerate(feature_subsets):\n",
        "        current_time = time.time()\n",
        "        elapsed = current_time - start_time\n",
        "\n",
        "        # Check time limit\n",
        "        if elapsed > max_time_seconds:\n",
        "            print(f\"\\nArrêt dû à la limite de temps ({max_time_minutes} min)\")\n",
        "            print(f\"Évalué {i} sous-ensembles sur {len(feature_subsets)}\")\n",
        "            break\n",
        "\n",
        "        # Calculate inconsistency rate\n",
        "        try:\n",
        "            ir = cm.calculate_inconsistency_rate(subset)\n",
        "            results.append({\n",
        "                'subset_id': i,\n",
        "                'features': subset,\n",
        "                'num_features': len(subset),\n",
        "                'inconsistency_rate': ir,\n",
        "                'consistency_score': 1 - ir\n",
        "            })\n",
        "\n",
        "            # Progress update every 50 evaluations\n",
        "            if (i + 1) % 50 == 0 or i == 0:\n",
        "                avg_time_per_eval = elapsed / (i + 1)\n",
        "                remaining_evals = len(feature_subsets) - (i + 1)\n",
        "                eta_seconds = remaining_evals * avg_time_per_eval\n",
        "                eta_minutes = eta_seconds / 60\n",
        "\n",
        "                progress_pct = ((i + 1) / len(feature_subsets)) * 100\n",
        "                print(f\"Progression: {progress_pct:.1f}% ({i + 1}/{len(feature_subsets)}) - \"\n",
        "                      f\"ETA: {eta_minutes:.1f} min - \"\n",
        "                      f\"Meilleur IR: {min(r['inconsistency_rate'] for r in results):.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur lors de l'évaluation du sous-ensemble {i}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if results:\n",
        "        df_results = pd.DataFrame(results)\n",
        "        return df_results.sort_values('inconsistency_rate')\n",
        "    else:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Example usage and demonstration\n",
        "def demonstrate_consistency_measure(data=None, class_column=None):\n",
        "    \"\"\"\n",
        "    Demonstrate the consistency measure with provided data or example data.\n",
        "    \"\"\"\n",
        "    if data is None:\n",
        "        # Create sample dataset\n",
        "        np.random.seed(42)\n",
        "\n",
        "        # Generate sample data with known inconsistencies\n",
        "        data = pd.DataFrame({\n",
        "            'feature1': [0, 0, 1, 1, 0, 0, 1, 1, 0, 1],\n",
        "            'feature2': [1, 1, 0, 0, 1, 1, 0, 0, 1, 0],\n",
        "            'feature3': [1, 0, 1, 0, 1, 0, 1, 0, 1, 1],\n",
        "            'class': [1, 0, 1, 0, 1, 1, 1, 0, 0, 1]  # Intentional inconsistencies\n",
        "        })\n",
        "        class_column = 'class'\n",
        "        print(\"Sample Dataset:\")\n",
        "        print(data)\n",
        "    else:\n",
        "        print(\"Using provided dataset:\")\n",
        "        print(f\"Shape: {data.shape}\")\n",
        "        print(f\"Features: {[col for col in data.columns if col != class_column]}\")\n",
        "        print(f\"Class column: {class_column}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "    # Initialize consistency measure\n",
        "    cm = ConsistencyMeasure(data, class_column)\n",
        "\n",
        "    # Generate feature subsets automatically from the dataset\n",
        "    print(f\"\\nGenerating feature subsets from {len(cm.feature_columns)} features...\")\n",
        "\n",
        "    if len(cm.feature_columns) <= 5:\n",
        "        # For small datasets, generate all possible subsets\n",
        "        print(\"Small dataset: generating all possible subsets\")\n",
        "        feature_subsets = generate_feature_subsets(cm.feature_columns, max_subset_size=len(cm.feature_columns))\n",
        "    elif len(cm.feature_columns) <= 15:\n",
        "        # For medium datasets, generate subsets up to size 4\n",
        "        print(\"Medium dataset: generating subsets up to size 4\")\n",
        "        feature_subsets = generate_feature_subsets(cm.feature_columns, max_subset_size=4)\n",
        "    else:\n",
        "        # For large datasets, use smart sampling\n",
        "        print(\"Large dataset: using smart subset sampling\")\n",
        "        feature_subsets = generate_smart_feature_subsets(cm.feature_columns, max_combinations=50)\n",
        "\n",
        "    print(f\"Generated {len(feature_subsets)} feature subsets\")\n",
        "\n",
        "    # Limit the number of subsets for demonstration\n",
        "    if len(feature_subsets) > 20:\n",
        "        print(f\"Limiting to first 20 subsets for demonstration...\")\n",
        "        feature_subsets = feature_subsets[:20]\n",
        "\n",
        "    print(\"\\nTesting feature subsets:\")\n",
        "    for i, subset in enumerate(feature_subsets[:10]):  # Show first 10\n",
        "        print(f\"  {i+1}: {subset}\")\n",
        "    if len(feature_subsets) > 10:\n",
        "        print(f\"  ... and {len(feature_subsets) - 10} more subsets\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "    print(\"\\nInconsistency Rates for Feature Subsets:\")\n",
        "    comparison = cm.compare_feature_subsets(feature_subsets)\n",
        "\n",
        "    # Show top 10 best subsets\n",
        "    print(\"\\nTop 10 best feature subsets (lowest inconsistency rate):\")\n",
        "    print(comparison.head(10).to_string(index=False))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "    # Detailed analysis for the best subset\n",
        "    best_subset = comparison.iloc[0]['features']\n",
        "    print(f\"\\nDetailed Analysis for Best Subset: {best_subset}\")\n",
        "    analysis = cm.analyze_pattern_details(best_subset)\n",
        "\n",
        "    print(f\"Total patterns: {analysis['total_patterns']}\")\n",
        "    print(f\"Inconsistent patterns: {analysis['inconsistent_patterns']}\")\n",
        "    print(f\"Inconsistency rate: {analysis['inconsistency_rate']:.4f}\")\n",
        "    print(f\"Consistency score: {1 - analysis['inconsistency_rate']:.4f}\")\n",
        "\n",
        "    if analysis['inconsistent_patterns'] > 0:\n",
        "        print(\"\\nInconsistent Patterns (first 5):\")\n",
        "        inconsistent_patterns = [detail for detail in analysis['pattern_details']\n",
        "                               if detail['is_inconsistent']][:5]\n",
        "        for i, detail in enumerate(inconsistent_patterns):\n",
        "            print(f\"  {i+1}. Pattern {detail['pattern']}: {detail['total_instances']} instances\")\n",
        "            print(f\"     Class distribution: {detail['class_distribution']}\")\n",
        "            print(f\"     Inconsistency count: {detail['inconsistency_count']}\")\n",
        "\n",
        "    return comparison\n",
        "\n",
        "# --- Main Function ---\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to load data, optimize features, and save results.\n",
        "    \"\"\"\n",
        "    input_file_path = '/content/GPCR-PrositeTRA0.arff'\n",
        "    output_file_path = '/content/GPCR-PrositeTRA0OptimizedRRHCIR.arff'\n",
        "\n",
        "    print(\"Chargement des données...\")\n",
        "    start_time = time.time()\n",
        "    data = read_arff_file(input_file_path)\n",
        "    load_time = time.time() - start_time\n",
        "    print(f\"Chargé {len(data)} instances avec {len(data.columns)-1} attributs en {load_time:.2f}s\")\n",
        "\n",
        "    # Assume last column is class (adjust if needed)\n",
        "    class_column = data.columns[-1]\n",
        "    print(f\"Colonne de classe: {class_column}\")\n",
        "    print(f\"Distribution des classes: {data[class_column].value_counts().to_dict()}\")\n",
        "\n",
        "    # Initialize consistency measure\n",
        "    print(\"\\nInitialisation de la mesure de cohérence...\")\n",
        "    cm = ConsistencyMeasure(data, class_column)\n",
        "\n",
        "    # Calculate initial inconsistency rate with all features\n",
        "    print(\"Calcul du taux d'incohérence initial...\")\n",
        "    initial_ir = cm.calculate_inconsistency_rate(cm.feature_columns)\n",
        "    print(f\"Taux d'incohérence initial (tous les {len(cm.feature_columns)} attributs): {initial_ir:.4f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PHASE 1: ÉVALUATION INTELLIGENTE DES SOUS-ENSEMBLES\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Generate feature subsets intelligently based on dataset size\n",
        "    if len(cm.feature_columns) <= 10:\n",
        "        print(\"Petit dataset: génération exhaustive de tous les sous-ensembles\")\n",
        "        feature_subsets = generate_feature_subsets(cm.feature_columns)\n",
        "        print(f\"Générés {len(feature_subsets)} sous-ensembles\")\n",
        "        subset_comparison = cm.compare_feature_subsets(feature_subsets)\n",
        "    else:\n",
        "        print(\"Grand dataset: génération intelligente de sous-ensembles\")\n",
        "        feature_subsets = generate_smart_feature_subsets(cm.feature_columns, max_combinations=500)\n",
        "\n",
        "        # Evaluate with progress tracking and time limit\n",
        "        subset_comparison = evaluate_subsets_with_progress(cm, feature_subsets, max_time_minutes=15)\n",
        "\n",
        "        if subset_comparison.empty:\n",
        "            print(\"Aucun résultat obtenu, utilisation d'un échantillon plus petit...\")\n",
        "            feature_subsets = generate_smart_feature_subsets(cm.feature_columns, max_combinations=100)\n",
        "            subset_comparison = evaluate_subsets_with_progress(cm, feature_subsets, max_time_minutes=5)\n",
        "\n",
        "    if not subset_comparison.empty:\n",
        "        print(f\"\\nÉvaluation terminée: {len(subset_comparison)} sous-ensembles testés\")\n",
        "\n",
        "        # Show top results\n",
        "        print(\"\\nTop 10 meilleurs sous-ensembles:\")\n",
        "        top_subsets = subset_comparison.head(10)\n",
        "        for idx, row in top_subsets.iterrows():\n",
        "            improvement = ((initial_ir - row['inconsistency_rate']) / initial_ir * 100)\n",
        "            print(f\"  {idx+1}: {row['num_features']} attributs, IR: {row['inconsistency_rate']:.4f} \"\n",
        "                  f\"(amélioration: {improvement:.1f}%)\")\n",
        "\n",
        "        # Get the best subset\n",
        "        best_exhaustive_subset = subset_comparison.iloc[0]['features']\n",
        "        best_exhaustive_ir = subset_comparison.iloc[0]['inconsistency_rate']\n",
        "\n",
        "        print(f\"\\nMeilleur sous-ensemble trouvé:\")\n",
        "        print(f\"  Nombre d'attributs: {len(best_exhaustive_subset)}\")\n",
        "        print(f\"  Taux d'incohérence: {best_exhaustive_ir:.4f}\")\n",
        "        print(f\"  Amélioration: {((initial_ir - best_exhaustive_ir) / initial_ir * 100):.2f}%\")\n",
        "    else:\n",
        "        print(\"Aucun résultat d'évaluation disponible\")\n",
        "        best_exhaustive_subset = cm.feature_columns[:10]  # Fallback\n",
        "        best_exhaustive_ir = cm.calculate_inconsistency_rate(best_exhaustive_subset)\n",
        "\n",
        "    # Initialize and run Random Restart Hill Climbing\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PHASE 2: OPTIMISATION RRHC\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Lancement de l'optimisation Random Restart Hill Climbing...\")\n",
        "\n",
        "    optimizer = RandomRestartHillClimbing(\n",
        "        consistency_measure=cm,\n",
        "        max_restarts=10,  # Reduced for large datasets\n",
        "        max_iterations=50,  # Reduced for large datasets\n",
        "        min_features=1\n",
        "    )\n",
        "\n",
        "    optimization_start = time.time()\n",
        "    result = optimizer.optimize(verbose=True)\n",
        "    optimization_time = time.time() - optimization_start\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"RÉSULTATS DE L'OPTIMISATION\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Meilleur sous-ensemble trouvé: {len(result['best_subset'])} attributs\")\n",
        "    print(f\"Attributs sélectionnés: {result['best_subset']}\")\n",
        "    print(f\"Taux d'incohérence optimisé: {result['best_ir']:.4f}\")\n",
        "    # Compare methods and select best solution\n",
        "    if not subset_comparison.empty:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"COMPARAISON DES MÉTHODES\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Évaluation intelligente:\")\n",
        "        print(f\"  Meilleur sous-ensemble: {len(best_exhaustive_subset)} attributs\")\n",
        "        print(f\"  Taux d'incohérence: {best_exhaustive_ir:.4f}\")\n",
        "        print(f\"\\nRandom Restart Hill Climbing:\")\n",
        "        print(f\"  Meilleur sous-ensemble: {len(result['best_subset'])} attributs\")\n",
        "        print(f\"  Taux d'incohérence: {result['best_ir']:.4f}\")\n",
        "        print(f\"  Évaluations totales: {result['total_evaluations']}\")\n",
        "        print(f\"  Temps d'optimisation: {optimization_time:.2f}s\")\n",
        "\n",
        "        if result['best_ir'] <= best_exhaustive_ir:\n",
        "            print(f\"✓ RRHC a trouvé une solution égale ou meilleure!\")\n",
        "            final_best_subset = result['best_subset']\n",
        "            final_best_ir = result['best_ir']\n",
        "        else:\n",
        "            print(f\"✓ L'évaluation intelligente a trouvé une meilleure solution!\")\n",
        "            improvement_possible = ((result['best_ir'] - best_exhaustive_ir) / result['best_ir'] * 100)\n",
        "            print(f\"  Amélioration de RRHC possible: {improvement_possible:.2f}%\")\n",
        "            final_best_subset = best_exhaustive_subset\n",
        "            final_best_ir = best_exhaustive_ir\n",
        "    else:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"RÉSULTATS RRHC SEULEMENT\")\n",
        "        print(\"=\"*60)\n",
        "        final_best_subset = result['best_subset']\n",
        "        final_best_ir = result['best_ir']\n",
        "        print(f\"Évaluations totales: {result['total_evaluations']}\")\n",
        "        print(f\"Temps d'optimisation: {optimization_time:.2f}s\")\n",
        "\n",
        "    print(f\"\\nSOLUTION FINALE:\")\n",
        "    print(f\"  Nombre d'attributs sélectionnés: {len(final_best_subset)}\")\n",
        "    print(f\"  Taux d'incohérence final: {final_best_ir:.4f}\")\n",
        "    print(f\"  Amélioration vs initial: {((initial_ir - final_best_ir) / initial_ir * 100):.2f}%\")\n",
        "    print(f\"  Réduction d'attributs: {((len(cm.feature_columns) - len(final_best_subset)) / len(cm.feature_columns) * 100):.1f}%\")\n",
        "\n",
        "    # Create optimized dataset with the final best subset\n",
        "    selected_columns = final_best_subset + [class_column]\n",
        "    optimized_data = data[selected_columns]\n",
        "\n",
        "    # Save optimized dataset\n",
        "    print(f\"\\nSauvegarde du dataset optimisé...\")\n",
        "    save_to_arff(optimized_data, output_file_path, \"optimized_data_RRHC\")\n",
        "    print(f\"Dataset sauvegardé: {output_file_path}\")\n",
        "\n",
        "    # Additional analysis\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ANALYSE DÉTAILLÉE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Pattern analysis for optimized subset\n",
        "    print(f\"\\nAnalyse des motifs pour le sous-ensemble optimisé:\")\n",
        "    pattern_analysis = cm.analyze_pattern_details(final_best_subset)\n",
        "    print(f\"  Motifs totaux: {pattern_analysis['total_patterns']}\")\n",
        "    print(f\"  Motifs incohérents: {pattern_analysis['inconsistent_patterns']}\")\n",
        "    print(f\"  Score de cohérence: {(1 - pattern_analysis['inconsistency_rate']):.4f}\")\n",
        "\n",
        "    # Show some inconsistent patterns if they exist\n",
        "    if pattern_analysis['inconsistent_patterns'] > 0:\n",
        "        print(f\"\\nExemples de motifs incohérents (premiers 3):\")\n",
        "        inconsistent_patterns = [detail for detail in pattern_analysis['pattern_details']\n",
        "                               if detail['is_inconsistent']][:3]\n",
        "        for i, detail in enumerate(inconsistent_patterns):\n",
        "            print(f\"  {i+1}. Motif {detail['pattern']}: {detail['total_instances']} instances\")\n",
        "            print(f\"     Distribution des classes: {detail['class_distribution']}\")\n",
        "            print(f\"     Nombre d'incohérences: {detail['inconsistency_count']}\")\n",
        "\n",
        "    # Show selected features\n",
        "    print(f\"\\nAttributs sélectionnés dans le meilleur sous-ensemble:\")\n",
        "    for i, feature in enumerate(final_best_subset, 1):\n",
        "        print(f\"  {i:2d}. {feature}\")\n",
        "\n",
        "    # Comparison by subset size (if we have evaluation data)\n",
        "    if not subset_comparison.empty and len(subset_comparison) > 10:\n",
        "        print(f\"\\nAnalyse par taille de sous-ensemble:\")\n",
        "        size_analysis = {}\n",
        "        for _, row in subset_comparison.iterrows():\n",
        "            size = row['num_features']\n",
        "            if size not in size_analysis:\n",
        "                size_analysis[size] = []\n",
        "            size_analysis[size].append(row['inconsistency_rate'])\n",
        "\n",
        "        for size in sorted(size_analysis.keys())[:10]:  # Show first 10 sizes\n",
        "            rates = size_analysis[size]\n",
        "            avg_rate = np.mean(rates)\n",
        "            min_rate = min(rates)\n",
        "            print(f\"  Taille {size:2d}: IR moyen = {avg_rate:.4f}, IR minimum = {min_rate:.4f} \"\n",
        "                  f\"({len(rates)} sous-ensembles)\")\n",
        "\n",
        "    return {\n",
        "        'optimized_data': optimized_data,\n",
        "        'best_subset': final_best_subset,\n",
        "        'best_ir': final_best_ir,\n",
        "        'initial_ir': initial_ir,\n",
        "        'improvement': ((initial_ir - final_best_ir) / initial_ir * 100),\n",
        "        'subset_comparison': subset_comparison,\n",
        "        'rrhc_result': result\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # You can also run the demonstration with sample data\n",
        "    # demonstrate_consistency_measure()\n",
        "\n",
        "    # Run the main optimization\n",
        "    main()"
      ]
    }
  ]
}