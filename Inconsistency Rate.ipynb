{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpIWw6CQtdmAyJ7Y0xHXuU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Feranie/Hierarchical-Classification-Project/blob/main/Inconsistency%20Rate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZIMGw-w8suR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd  # Importa biblioteca pandas para manipulação de dados tabulares\n",
        "import numpy as np  # Importa numpy para operações matemáticas e arrays\n",
        "import re  # Importa regex para processamento de strings e padrões\n",
        "import time  # Importa time para medir tempos de execução\n",
        "import random  # Importa random para geração de números aleatórios\n",
        "from collections import defaultdict, Counter  # Importa estruturas de dados especializadas\n",
        "from typing import List, Tuple, Dict, Any  # Importa tipos para anotações de tipo\n",
        "\n",
        "# --- Leitura de Arquivo ARFF ---\n",
        "def read_arff_file(file_path):\n",
        "    \"\"\"\n",
        "    Lê arquivo ARFF e retorna DataFrame pandas.\n",
        "\n",
        "    Args:\n",
        "        file_path: Caminho do arquivo ARFF\n",
        "\n",
        "    Returns:\n",
        "        DataFrame pandas com os dados\n",
        "    \"\"\"\n",
        "    data = []  # Inicializa lista vazia para armazenar linhas de dados\n",
        "    attributes = []  # Inicializa lista vazia para armazenar nomes dos atributos\n",
        "    current_section = None  # Variável para rastrear em qual seção do ARFF estamos\n",
        "\n",
        "    # Abre o arquivo em modo leitura com codificação UTF-8\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:  # Para cada linha do arquivo\n",
        "            line = line.strip()  # Remove espaços em branco no início e fim\n",
        "            if not line or line.startswith('%'):  # Se linha vazia ou comentário\n",
        "                continue  # Pula para próxima linha\n",
        "\n",
        "            if '@relation' in line.lower():  # Se encontrou declaração @relation\n",
        "                current_section = 'relation'  # Marca seção atual como 'relation'\n",
        "            elif '@attribute' in line.lower():  # Se encontrou declaração @attribute\n",
        "                current_section = 'attribute'  # Marca seção atual como 'attribute'\n",
        "                # Usa regex para extrair nome do atributo da linha\n",
        "                match = re.match(r'@attribute\\s+([^\\s]+)\\s+.*', line, re.IGNORECASE)\n",
        "                if match:  # Se regex encontrou correspondência\n",
        "                    attributes.append(match.group(1))  # Adiciona nome do atributo à lista\n",
        "            elif '@data' in line.lower():  # Se encontrou declaração @data\n",
        "                current_section = 'data'  # Marca seção atual como 'data'\n",
        "            elif current_section == 'data':  # Se estamos na seção de dados\n",
        "                # Usa regex para dividir linha por vírgulas (mantendo valores entre aspas juntos)\n",
        "                values = re.findall(r'[^,]+(?:,(?=[^,]$))?', line)\n",
        "                # Remove aspas e espaços de cada valor\n",
        "                values = [v.strip('\" ') for v in values]\n",
        "                if len(values) == len(attributes):  # Se número de valores bate com atributos\n",
        "                    data.append(values)  # Adiciona linha aos dados\n",
        "\n",
        "    # Converte lista de dados para DataFrame pandas com nomes das colunas\n",
        "    return pd.DataFrame(data, columns=attributes)\n",
        "\n",
        "# --- Salvamento de Arquivo ARFF ---\n",
        "def save_to_arff(df, file_path, relation_name=\"filtered_data\"):\n",
        "    \"\"\"\n",
        "    Salva DataFrame no formato de arquivo ARFF.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame pandas a ser salvo\n",
        "        file_path: Caminho onde salvar o arquivo\n",
        "        relation_name: Nome da relação no ARFF\n",
        "    \"\"\"\n",
        "    # Abre arquivo em modo escrita com codificação UTF-8\n",
        "    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "        # Escreve declaração @relation\n",
        "        f.write(f\"@relation {relation_name}\\n\\n\")\n",
        "\n",
        "        # Para cada coluna do DataFrame\n",
        "        for column in df.columns:\n",
        "            unique_values = df[column].unique()  # Obtém valores únicos da coluna\n",
        "            # Se todos os valores são strings (atributo categórico)\n",
        "            if all(isinstance(val, str) for val in unique_values):\n",
        "                vals = ','.join(sorted(set(unique_values)))  # Junta valores únicos separados por vírgula\n",
        "                # Escreve declaração de atributo categórico\n",
        "                f.write(f\"@attribute {column} {{{vals}}}\\n\")\n",
        "            else:  # Se atributo numérico\n",
        "                # Escreve declaração de atributo numérico\n",
        "                f.write(f\"@attribute {column} numeric\\n\")\n",
        "\n",
        "        f.write(\"\\n@data\\n\")  # Escreve marcador de início dos dados\n",
        "        # Para cada linha do DataFrame\n",
        "        for _, row in df.iterrows():\n",
        "            # Escreve linha com valores separados por vírgula\n",
        "            f.write(\",\".join(map(str, row)) + \"\\n\")\n",
        "\n",
        "class ConsistencyMeasure:\n",
        "    \"\"\"\n",
        "    Implementação da medida de consistência (taxa de inconsistência) conforme descrito em:\n",
        "    M. Dash, H. Liu / Artificial Intelligence 151 (2003) 155–176\n",
        "\n",
        "    Esta métrica avalia a qualidade de um subconjunto de atributos medindo\n",
        "    quantas instâncias têm o mesmo padrão de atributos mas classes diferentes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data: pd.DataFrame, class_column: str):\n",
        "        \"\"\"\n",
        "        Inicializa o calculador de medida de consistência.\n",
        "\n",
        "        Args:\n",
        "            data: DataFrame contendo o dataset\n",
        "            class_column: Nome da coluna de classe/alvo\n",
        "        \"\"\"\n",
        "        self.data = data  # Armazena o DataFrame completo\n",
        "        self.class_column = class_column  # Armazena nome da coluna de classe\n",
        "        # Cria lista de colunas de atributos (todas exceto a classe)\n",
        "        self.feature_columns = [col for col in data.columns if col != class_column]\n",
        "        self.total_instances = len(data)  # Armazena número total de instâncias\n",
        "\n",
        "    def calculate_inconsistency_rate(self, feature_subset: List[str]) -> float:\n",
        "        \"\"\"\n",
        "        Calcula a taxa de inconsistência para um dado subconjunto de atributos.\n",
        "\n",
        "        Taxa de Inconsistência (IR) = Número de instâncias inconsistentes / Total de instâncias\n",
        "\n",
        "        Uma instância é inconsistente quando:\n",
        "        - Tem o mesmo padrão de valores de atributos que outras instâncias\n",
        "        - Mas pertence a uma classe diferente da classe majoritária desse padrão\n",
        "\n",
        "        Args:\n",
        "            feature_subset: Lista de nomes de atributos a considerar\n",
        "\n",
        "        Returns:\n",
        "            Taxa de inconsistência (IR) para o subconjunto de atributos\n",
        "        \"\"\"\n",
        "        if not feature_subset:  # Se subconjunto vazio\n",
        "            return 0.0  # Retorna 0 (sem atributos = sem inconsistência mensurável)\n",
        "\n",
        "        # Valida se todos os atributos do subconjunto existem no dataset\n",
        "        invalid_features = [f for f in feature_subset if f not in self.feature_columns]\n",
        "        if invalid_features:  # Se há atributos inválidos\n",
        "            # Lança erro com lista de atributos inválidos\n",
        "            raise ValueError(f\"Atributos inválidos: {invalid_features}\")\n",
        "\n",
        "        # Agrupa instâncias por padrão (combinação de valores de atributos)\n",
        "        patterns = self._group_by_pattern(feature_subset)\n",
        "\n",
        "        # Calcula contagem de inconsistências para cada padrão\n",
        "        total_inconsistency_count = 0  # Inicializa contador total\n",
        "\n",
        "        # Para cada padrão único e suas instâncias\n",
        "        for pattern, instances in patterns.items():\n",
        "            # Calcula inconsistências deste padrão específico\n",
        "            inconsistency_count = self._calculate_pattern_inconsistency(instances)\n",
        "            total_inconsistency_count += inconsistency_count  # Acumula no total\n",
        "\n",
        "        # Calcula taxa de inconsistência = inconsistências / total de instâncias\n",
        "        inconsistency_rate = total_inconsistency_count / self.total_instances\n",
        "        return inconsistency_rate  # Retorna taxa (valor entre 0 e 1)\n",
        "\n",
        "    def _group_by_pattern(self, feature_subset: List[str]) -> Dict[Tuple, List[Any]]:\n",
        "        \"\"\"\n",
        "        Agrupa instâncias por seu padrão (combinações de valores de atributos).\n",
        "\n",
        "        Exemplo:\n",
        "        Se feature_subset = ['idade', 'salário']\n",
        "        Padrão (25, 3000) pode ter instâncias com classes [1, 1, 0]\n",
        "        Padrão (30, 4000) pode ter instâncias com classes [0, 0]\n",
        "\n",
        "        Args:\n",
        "            feature_subset: Lista de nomes de atributos\n",
        "\n",
        "        Returns:\n",
        "            Dicionário mapeando padrões para listas de rótulos de classe\n",
        "        \"\"\"\n",
        "        # Cria dicionário que automaticamente cria listas vazias para novas chaves\n",
        "        patterns = defaultdict(list)\n",
        "\n",
        "        # Para cada linha no dataset\n",
        "        for _, row in self.data.iterrows():\n",
        "            # Cria tupla com valores dos atributos selecionados\n",
        "            # Exemplo: (25, 3000) para atributos ['idade', 'salário']\n",
        "            pattern = tuple(row[feature] for feature in feature_subset)\n",
        "            class_label = row[self.class_column]  # Obtém rótulo de classe desta instância\n",
        "            patterns[pattern].append(class_label)  # Adiciona classe à lista deste padrão\n",
        "\n",
        "        return patterns  # Retorna dicionário padrão -> lista de classes\n",
        "\n",
        "    def _calculate_pattern_inconsistency(self, class_labels: List[Any]) -> int:\n",
        "        \"\"\"\n",
        "        Calcula contagem de inconsistências para um único padrão.\n",
        "\n",
        "        Exemplo:\n",
        "        Se um padrão tem classes [1, 1, 1, 0, 0]:\n",
        "        - Classe majoritária: 1 (aparece 3 vezes)\n",
        "        - Inconsistências: 2 (as duas instâncias com classe 0)\n",
        "\n",
        "        Args:\n",
        "            class_labels: Lista de rótulos de classe para instâncias com o mesmo padrão\n",
        "\n",
        "        Returns:\n",
        "            Contagem de inconsistências para este padrão\n",
        "        \"\"\"\n",
        "        if len(class_labels) <= 1:  # Se há 1 ou 0 instâncias com este padrão\n",
        "            return 0  # Não há inconsistência (precisa de pelo menos 2 para conflito)\n",
        "\n",
        "        # Conta ocorrências de cada rótulo de classe\n",
        "        # Exemplo: Counter([1, 1, 1, 0, 0]) retorna {1: 3, 0: 2}\n",
        "        label_counts = Counter(class_labels)\n",
        "\n",
        "        # Encontra contagem máxima (classe mais frequente)\n",
        "        # Exemplo: max({1: 3, 0: 2}.values()) retorna 3\n",
        "        max_count = max(label_counts.values())\n",
        "\n",
        "        # Contagem de inconsistências = total de instâncias - instâncias da classe majoritária\n",
        "        # Exemplo: 5 instâncias - 3 (maioria) = 2 inconsistências\n",
        "        inconsistency_count = len(class_labels) - max_count\n",
        "\n",
        "        return inconsistency_count  # Retorna número de instâncias inconsistentes\n",
        "\n",
        "    def analyze_pattern_details(self, feature_subset: List[str]) -> Dict:\n",
        "        \"\"\"\n",
        "        Fornece análise detalhada de padrões e suas inconsistências.\n",
        "\n",
        "        Útil para entender quais padrões são problemáticos e por quê.\n",
        "\n",
        "        Args:\n",
        "            feature_subset: Lista de nomes de atributos a analisar\n",
        "\n",
        "        Returns:\n",
        "            Dicionário com análise detalhada dos padrões\n",
        "        \"\"\"\n",
        "        # Agrupa instâncias por padrão\n",
        "        patterns = self._group_by_pattern(feature_subset)\n",
        "\n",
        "        # Inicializa dicionário de análise com estatísticas\n",
        "        analysis = {\n",
        "            'total_patterns': len(patterns),  # Número total de padrões únicos\n",
        "            'inconsistent_patterns': 0,  # Contador de padrões com inconsistências\n",
        "            'pattern_details': [],  # Lista para detalhes de cada padrão\n",
        "            'total_inconsistency_count': 0  # Contador total de inconsistências\n",
        "        }\n",
        "\n",
        "        # Para cada padrão e suas classes\n",
        "        for pattern, class_labels in patterns.items():\n",
        "            label_counts = Counter(class_labels)  # Conta ocorrências de cada classe\n",
        "            # Calcula inconsistências deste padrão\n",
        "            inconsistency_count = self._calculate_pattern_inconsistency(class_labels)\n",
        "\n",
        "            # Cria dicionário com informações sobre este padrão\n",
        "            pattern_info = {\n",
        "                'pattern': pattern,  # Valores dos atributos (ex: (25, 3000))\n",
        "                'total_instances': len(class_labels),  # Quantas instâncias têm este padrão\n",
        "                'class_distribution': dict(label_counts),  # Distribuição de classes {1: 3, 0: 2}\n",
        "                'inconsistency_count': inconsistency_count,  # Número de inconsistências\n",
        "                'is_inconsistent': inconsistency_count > 0  # Boolean: tem inconsistência?\n",
        "            }\n",
        "\n",
        "            analysis['pattern_details'].append(pattern_info)  # Adiciona à lista de detalhes\n",
        "            analysis['total_inconsistency_count'] += inconsistency_count  # Acumula total\n",
        "\n",
        "            if inconsistency_count > 0:  # Se este padrão tem inconsistências\n",
        "                analysis['inconsistent_patterns'] += 1  # Incrementa contador\n",
        "\n",
        "        # Calcula taxa de inconsistência total\n",
        "        analysis['inconsistency_rate'] = analysis['total_inconsistency_count'] / self.total_instances\n",
        "\n",
        "        return analysis  # Retorna dicionário com análise completa\n",
        "\n",
        "    def compare_feature_subsets(self, feature_subsets: List[List[str]]) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Compara taxas de inconsistência entre múltiplos subconjuntos de atributos.\n",
        "\n",
        "        Útil para avaliar qual combinação de atributos é mais consistente.\n",
        "\n",
        "        Args:\n",
        "            feature_subsets: Lista de listas de atributos para comparar\n",
        "\n",
        "        Returns:\n",
        "            DataFrame com resultados da comparação ordenados por IR\n",
        "        \"\"\"\n",
        "        results = []  # Inicializa lista para resultados\n",
        "\n",
        "        # Para cada subconjunto de atributos\n",
        "        for i, subset in enumerate(feature_subsets):\n",
        "            ir = self.calculate_inconsistency_rate(subset)  # Calcula IR\n",
        "            # Adiciona resultado à lista\n",
        "            results.append({\n",
        "                'subset_id': i,  # ID do subconjunto\n",
        "                'features': subset,  # Lista de atributos\n",
        "                'num_features': len(subset),  # Quantidade de atributos\n",
        "                'inconsistency_rate': ir,  # Taxa de inconsistência\n",
        "                'consistency_score': 1 - ir  # Score de consistência (quanto maior, melhor)\n",
        "            })\n",
        "\n",
        "        # Converte lista para DataFrame e ordena por IR (menor primeiro = melhor)\n",
        "        return pd.DataFrame(results).sort_values('inconsistency_rate')\n",
        "\n",
        "class RandomRestartHillClimbing:\n",
        "    \"\"\"\n",
        "    Algoritmo Random Restart Hill Climbing para encontrar subconjunto ótimo de atributos\n",
        "    que minimiza a taxa de inconsistência (IR).\n",
        "\n",
        "    O algoritmo:\n",
        "    1. Inicia de múltiplos pontos aleatórios (restarts)\n",
        "    2. Para cada início, executa hill climbing (busca local)\n",
        "    3. Aceita mudanças que melhoram o IR\n",
        "    4. Para quando não há mais melhorias\n",
        "    5. Retorna o melhor resultado de todos os restarts\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, consistency_measure: ConsistencyMeasure, max_restarts=10,\n",
        "                 max_iterations=100, min_features=1):\n",
        "        \"\"\"\n",
        "        Inicializa otimizador RRHC.\n",
        "\n",
        "        Args:\n",
        "            consistency_measure: Instância de ConsistencyMeasure\n",
        "            max_restarts: Número máximo de reinícios aleatórios\n",
        "            max_iterations: Máximo de iterações por restart\n",
        "            min_features: Número mínimo de atributos a manter\n",
        "        \"\"\"\n",
        "        self.cm = consistency_measure  # Armazena calculador de consistência\n",
        "        self.max_restarts = max_restarts  # Armazena número de restarts\n",
        "        self.max_iterations = max_iterations  # Armazena máximo de iterações\n",
        "        self.min_features = min_features  # Armazena mínimo de atributos\n",
        "        self.all_features = self.cm.feature_columns.copy()  # Copia lista de todos atributos\n",
        "\n",
        "    def optimize(self, verbose=True):\n",
        "        \"\"\"\n",
        "        Executa Random Restart Hill Climbing para encontrar subconjunto ótimo.\n",
        "\n",
        "        Args:\n",
        "            verbose: Imprimir informações de progresso\n",
        "\n",
        "        Returns:\n",
        "            Dicionário com resultados da otimização\n",
        "        \"\"\"\n",
        "        best_subset = None  # Inicializa melhor subconjunto como None\n",
        "        best_ir = float('inf')  # Inicializa melhor IR como infinito (pior possível)\n",
        "        best_restart = -1  # Inicializa índice do melhor restart\n",
        "        all_results = []  # Lista para armazenar resultados de todos restarts\n",
        "\n",
        "        # Para cada restart\n",
        "        for restart in range(self.max_restarts):\n",
        "            if verbose:  # Se modo verbose ativado\n",
        "                print(f\"\\n--- Restart {restart + 1}/{self.max_restarts} ---\")\n",
        "\n",
        "            # Gera subconjunto inicial aleatório\n",
        "            initial_size = random.randint(self.min_features, len(self.all_features))  # Tamanho aleatório\n",
        "            current_subset = random.sample(self.all_features, initial_size)  # Seleciona atributos aleatórios\n",
        "            current_ir = self.cm.calculate_inconsistency_rate(current_subset)  # Calcula IR inicial\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Subconjunto inicial: {len(current_subset)} atributos, IR: {current_ir:.4f}\")\n",
        "\n",
        "            # Hill climbing a partir deste ponto inicial\n",
        "            result = self._hill_climb(current_subset, current_ir, verbose)\n",
        "            all_results.append(result)  # Adiciona resultado à lista\n",
        "\n",
        "            # Atualiza melhor global se este restart foi melhor\n",
        "            if result['final_ir'] < best_ir:\n",
        "                best_ir = result['final_ir']  # Atualiza melhor IR\n",
        "                best_subset = result['final_subset'].copy()  # Atualiza melhor subconjunto\n",
        "                best_restart = restart  # Marca qual restart foi o melhor\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Final: {len(result['final_subset'])} atributos, IR: {result['final_ir']:.4f}\")\n",
        "\n",
        "        # Retorna dicionário com todos os resultados\n",
        "        return {\n",
        "            'best_subset': best_subset,  # Melhor subconjunto encontrado\n",
        "            'best_ir': best_ir,  # Melhor IR encontrado\n",
        "            'best_restart': best_restart,  # Qual restart foi o melhor\n",
        "            'all_results': all_results,  # Resultados de todos os restarts\n",
        "            'total_evaluations': sum(r['evaluations'] for r in all_results)  # Total de avaliações\n",
        "        }\n",
        "\n",
        "    def _hill_climb(self, initial_subset, initial_ir, verbose=False):\n",
        "        \"\"\"\n",
        "        Executa hill climbing a partir de um subconjunto inicial.\n",
        "\n",
        "        Aceita mudanças que diminuem o IR (melhoram a consistência).\n",
        "        Para quando não há mais melhorias possíveis.\n",
        "        \"\"\"\n",
        "        current_subset = initial_subset.copy()  # Copia subconjunto inicial\n",
        "        current_ir = initial_ir  # Armazena IR inicial\n",
        "        iteration = 0  # Contador de iterações\n",
        "        evaluations = 1  # Contador de avaliações (conta avaliação inicial)\n",
        "        improvements = 0  # Contador de melhorias\n",
        "\n",
        "        # Loop até máximo de iterações\n",
        "        while iteration < self.max_iterations:\n",
        "            iteration += 1  # Incrementa contador\n",
        "            improved = False  # Flag para indicar se houve melhoria\n",
        "\n",
        "            # Gera todos os vizinhos (adiciona/remove um atributo)\n",
        "            neighbors = self._generate_neighbors(current_subset)\n",
        "\n",
        "            # Avalia todos os vizinhos\n",
        "            for neighbor in neighbors:\n",
        "                if len(neighbor) < self.min_features:  # Se vizinho tem menos que mínimo\n",
        "                    continue  # Pula este vizinho\n",
        "\n",
        "                neighbor_ir = self.cm.calculate_inconsistency_rate(neighbor)  # Calcula IR do vizinho\n",
        "                evaluations += 1  # Incrementa contador de avaliações\n",
        "\n",
        "                # Aceita se melhor (IR menor)\n",
        "                if neighbor_ir < current_ir:\n",
        "                    current_subset = neighbor.copy()  # Atualiza subconjunto atual\n",
        "                    current_ir = neighbor_ir  # Atualiza IR atual\n",
        "                    improved = True  # Marca que houve melhoria\n",
        "                    improvements += 1  # Incrementa contador de melhorias\n",
        "\n",
        "                    # Log a cada 10 iterações\n",
        "                    if verbose and iteration % 10 == 0:\n",
        "                        print(f\"  Iteração {iteration}: {len(current_subset)} atributos, IR: {current_ir:.4f}\")\n",
        "                    break  # Aceita primeira melhoria (greedy)\n",
        "\n",
        "            if not improved:  # Se não houve melhoria\n",
        "                break  # Para o hill climbing (ótimo local encontrado)\n",
        "\n",
        "        # Retorna dicionário com resultados deste hill climb\n",
        "        return {\n",
        "            'final_subset': current_subset,  # Subconjunto final\n",
        "            'final_ir': current_ir,  # IR final\n",
        "            'iterations': iteration,  # Número de iterações\n",
        "            'evaluations': evaluations,  # Número de avaliações\n",
        "            'improvements': improvements  # Número de melhorias\n",
        "        }\n",
        "\n",
        "    def _generate_neighbors(self, current_subset):\n",
        "        \"\"\"\n",
        "        Gera subconjuntos vizinhos adicionando/removendo um atributo.\n",
        "\n",
        "        Vizinhos são subconjuntos que diferem em exatamente um atributo.\n",
        "        \"\"\"\n",
        "        neighbors = []  # Inicializa lista de vizinhos\n",
        "        current_set = set(current_subset)  # Converte para conjunto para busca rápida\n",
        "\n",
        "        # Remove um atributo (se possível)\n",
        "        if len(current_subset) > self.min_features:  # Se há margem para remover\n",
        "            for feature in current_subset:  # Para cada atributo no subconjunto atual\n",
        "                # Cria vizinho removendo este atributo\n",
        "                neighbor = [f for f in current_subset if f != feature]\n",
        "                neighbors.append(neighbor)  # Adiciona à lista de vizinhos\n",
        "\n",
        "        # Adiciona um atributo\n",
        "        # Encontra atributos que NÃO estão no subconjunto atual\n",
        "        available_features = [f for f in self.all_features if f not in current_set]\n",
        "        for feature in available_features:  # Para cada atributo disponível\n",
        "            neighbor = current_subset + [feature]  # Cria vizinho adicionando atributo\n",
        "            neighbors.append(neighbor)  # Adiciona à lista de vizinhos\n",
        "\n",
        "        return neighbors  # Retorna lista de todos os vizinhos\n",
        "\n",
        "def generate_smart_feature_subsets(feature_columns, max_combinations=200, include_large_subsets=True):\n",
        "    \"\"\"\n",
        "    Gera seleção inteligente de subconjuntos de atributos para datasets grandes.\n",
        "    Usa amostragem estratégica para evitar explosão combinatorial.\n",
        "\n",
        "    Estratégia:\n",
        "    1. Inclui todos os atributos individuais\n",
        "    2. Amostra pares de atributos\n",
        "    3. Amostra alguns subconjuntos maiores (3, 4, 5, 10, 15, 20)\n",
        "    4. Adiciona subconjuntos aleatórios de tamanhos variados\n",
        "\n",
        "    Args:\n",
        "        feature_columns: Lista de nomes de atributos\n",
        "        max_combinations: Número máximo de subconjuntos a gerar\n",
        "        include_large_subsets: Se deve incluir subconjuntos maiores\n",
        "\n",
        "    Returns:\n",
        "        Lista de subconjuntos de atributos\n",
        "    \"\"\"\n",
        "    from itertools import combinations  # Importa função para gerar combinações\n",
        "    import random  # Já importado no topo, mas explicitamente aqui\n",
        "\n",
        "    feature_subsets = []  # Inicializa lista de subconjuntos\n",
        "    n_features = len(feature_columns)  # Número total de atributos\n",
        "\n",
        "    print(f\"Geração inteligente de subconjuntos para {n_features} atributos...\")\n",
        "\n",
        "    # Sempre inclui atributos individuais\n",
        "    print(\"Adicionando atributos individuais...\")\n",
        "    for feature in feature_columns:  # Para cada atributo\n",
        "        feature_subsets.append([feature])  # Adiciona como subconjunto de tamanho 1\n",
        "\n",
        "    remaining_budget = max_combinations - len(feature_subsets)  # Calcula orçamento restante\n",
        "\n",
        "    # Adiciona pares estrategicamente\n",
        "    if n_features <= 20:  # Se dataset pequeno\n",
        "        # Suficientemente pequeno para incluir todos os pares\n",
        "        print(\"Adicionando todos os pares...\")\n",
        "        for pair in combinations(feature_columns, 2):  # Gera todas as combinações de 2\n",
        "            feature_subsets.append(list(pair))  # Adiciona par à lista\n",
        "            remaining_budget -= 1  # Decrementa orçamento\n",
        "            if remaining_budget <= 0:  # Se orçamento esgotado\n",
        "                break  # Para de adicionar\n",
        "    else:  # Se dataset grande\n",
        "        # Amostra pares aleatoriamente\n",
        "        pair_budget = min(100, remaining_budget // 2)  # Reserva até 100 ou metade do orçamento\n",
        "        print(f\"Amostrando {pair_budget} pares...\")\n",
        "        all_pairs = list(combinations(feature_columns, 2))  # Gera todos os pares possíveis\n",
        "        sampled_pairs = random.sample(all_pairs, min(pair_budget, len(all_pairs)))  # Amostra\n",
        "        for pair in sampled_pairs:  # Para cada par amostrado\n",
        "            feature_subsets.append(list(pair))  # Adiciona à lista\n",
        "        remaining_budget -= len(sampled_pairs)  # Decrementa orçamento\n",
        "\n",
        "    # Adiciona subconjuntos maiores se solicitado e há orçamento\n",
        "    if include_large_subsets and remaining_budget > 0:\n",
        "        for size in [3, 4, 5, 10, 15, 20]:  # Para cada tamanho específico\n",
        "            if size > n_features or remaining_budget <= 0:  # Se tamanho inválido ou sem orçamento\n",
        "                break  # Para\n",
        "\n",
        "            # Calcula orçamento para este tamanho\n",
        "            size_budget = min(20, remaining_budget // (6 - (size - 3)),\n",
        "                            remaining_budget if size >= 10 else remaining_budget)\n",
        "\n",
        "            if size_budget > 0:  # Se há orçamento\n",
        "                print(f\"Amostrando {size_budget} subconjuntos de tamanho {size}...\")\n",
        "                all_combinations = list(combinations(feature_columns, size))  # Gera todas combinações\n",
        "                sample_size = min(size_budget, len(all_combinations))  # Limita ao disponível\n",
        "                sampled_combinations = random.sample(all_combinations, sample_size)  # Amostra\n",
        "\n",
        "                for combo in sampled_combinations:  # Para cada combinação amostrada\n",
        "                    feature_subsets.append(list(combo))  # Adiciona à lista\n",
        "\n",
        "                remaining_budget -= sample_size  # Decrementa orçamento\n",
        "\n",
        "    # Adiciona subconjuntos aleatórios de tamanhos variados\n",
        "    if remaining_budget > 0:\n",
        "        print(f\"Adicionando {remaining_budget} subconjuntos aleatórios...\")\n",
        "        for _ in range(remaining_budget):  # Para o orçamento restante\n",
        "            size = random.randint(2, min(25, n_features))  # Tamanho aleatório entre 2 e 25\n",
        "            random_subset = random.sample(feature_columns, size)  # Amostra atributos aleatórios\n",
        "            feature_subsets.append(random_subset)  # Adiciona à lista\n",
        "\n",
        "    print(f\"Total gerado: {len(feature_subsets)} subconjuntos\")\n",
        "    return feature_subsets  # Retorna lista completa de subconjuntos\n",
        "\n",
        "# --- Função Principal ---\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Função principal para carregar dados, otimizar atributos e salvar resultados.\n",
        "\n",
        "    Fluxo:\n",
        "    1. Carrega dados do arquivo ARFF\n",
        "    2. Inicializa medida de consistência\n",
        "    3. Calcula IR inicial com todos atributos\n",
        "    4. Gera e avalia subconjuntos inteligentemente\n",
        "    5. Executa otimização RRHC\n",
        "    6. Compara resultados e seleciona melhor solução\n",
        "    7. Salva dataset otimizado\n",
        "    \"\"\"\n",
        "    # Define caminhos dos arquivos\n",
        "    input_file_path = '/content/GPCR-PrositeTRA0.arff'  # Arquivo de entrada\n",
        "    output_file_path = '/content/GPCR-PrositeTRA0OptimizedRRHCIR.arff'  # Arquivo de saída\n",
        "\n",
        "    print(\"Carregando dados...\")  # Log\n",
        "    start_time = time.time()  # Marca tempo de início\n",
        "    data = read_arff_file(input_file_path)  # Carrega dados do ARFF\n",
        "    load_time = time.time() - start_time  # Calcula tempo de carregamento\n",
        "    # Imprime estatísticas de carregamento\n",
        "    print(f\"Carregados {len(data)} instâncias com {len(data.columns)-1} atributos em {load_time:.2f}s\")\n",
        "\n",
        "    # Assume que última coluna é a classe (ajuste se necessário)\n",
        "    class_column = data.columns[-1]  # Obtém nome da última coluna\n",
        "    print(f\"Coluna de classe: {class_column}\")  # Log\n",
        "    # Imprime distribuição de classes\n",
        "    print(f\"Distribuição de classes: {data[class_column].value_counts().to_dict()}\")\n",
        "\n",
        "    # Inicializa medida de consistência\n",
        "    print(\"\\nInicializando medida de consistência...\")\n",
        "    cm = ConsistencyMeasure(data, class_column)  # Cria instância\n",
        "\n",
        "    # Calcula taxa de inconsistência inicial com todos os atributos\n",
        "    print(\"Calculando taxa de inconsistência inicial...\")\n",
        "    initial_ir = cm.calculate_inconsistency_rate(cm.feature_columns)  # Calcula IR\n",
        "    # Imprime IR inicial\n",
        "    print(f\"Taxa de inconsistência inicial (todos os {len(cm.feature_columns)} atributos): {initial_ir:.4f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FASE 1: AVALIAÇÃO INTELIGENTE DE SUBCONJUNTOS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Gera subconjuntos inteligentemente baseado no tamanho do dataset\n",
        "    print(\"Dataset grande: geração inteligente de subconjuntos\")\n",
        "    # Gera até 500 subconjuntos diferentes para avaliar\n",
        "    feature_subsets = generate_smart_feature_subsets(cm.feature_columns, max_combinations=500)\n",
        "\n",
        "    # Avalia subconjuntos com rastreamento de progresso\n",
        "    print(\"\\nAvaliando subconjuntos gerados...\")\n",
        "    results = []  # Lista para armazenar resultados\n",
        "    start_eval = time.time()  # Marca tempo de início da avaliação\n",
        "\n",
        "    # Para cada subconjunto gerado\n",
        "    for i, subset in enumerate(feature_subsets):\n",
        "        try:\n",
        "            ir = cm.calculate_inconsistency_rate(subset)  # Calcula IR do subconjunto\n",
        "            # Adiciona resultado à lista\n",
        "            results.append({\n",
        "                'subset_id': i,  # ID do subconjunto\n",
        "                'features': subset,  # Lista de atributos\n",
        "                'num_features': len(subset),  # Número de atributos\n",
        "                'inconsistency_rate': ir,  # Taxa de inconsistência\n",
        "                'consistency_score': 1 - ir  # Score de consistência\n",
        "            })\n",
        "\n",
        "            # Atualiza progresso a cada 50 avaliações\n",
        "            if (i + 1) % 50 == 0:\n",
        "                elapsed = time.time() - start_eval  # Tempo decorrido\n",
        "                avg_time = elapsed / (i + 1)  # Tempo médio por avaliação\n",
        "                remaining = len(feature_subsets) - (i + 1)  # Avaliações restantes\n",
        "                eta = remaining * avg_time / 60  # Tempo estimado em minutos\n",
        "                best_so_far = min(r['inconsistency_rate'] for r in results)  # Melhor IR até agora\n",
        "                # Imprime progresso\n",
        "                print(f\"Progresso: {((i+1)/len(feature_subsets)*100):.1f}% ({i+1}/{len(feature_subsets)}) - \"\n",
        "                      f\"ETA: {eta:.1f} min - Melhor IR: {best_so_far:.4f}\")\n",
        "        except Exception as e:  # Se erro ao avaliar\n",
        "            print(f\"Erro ao avaliar subconjunto {i}: {e}\")  # Log do erro\n",
        "            continue  # Pula para próximo\n",
        "\n",
        "    # Converte resultados para DataFrame e ordena por IR\n",
        "    subset_comparison = pd.DataFrame(results).sort_values('inconsistency_rate')\n",
        "    print(f\"\\nAvaliação concluída: {len(subset_comparison)} subconjuntos testados\")\n",
        "\n",
        "    # Mostra top 10 resultados\n",
        "    print(\"\\nTop 10 melhores subconjuntos:\")\n",
        "    top_subsets = subset_comparison.head(10)  # Pega os 10 melhores\n",
        "    for idx, row in top_subsets.iterrows():  # Para cada um\n",
        "        # Calcula melhoria percentual\n",
        "        improvement = ((initial_ir - row['inconsistency_rate']) / initial_ir * 100)\n",
        "        # Imprime informações\n",
        "        print(f\"  {idx+1}: {row['num_features']} atributos, IR: {row['inconsistency_rate']:.4f} \"\n",
        "              f\"(melhoria: {improvement:.1f}%)\")\n",
        "\n",
        "    # Obtém melhor subconjunto da avaliação\n",
        "    best_exhaustive_subset = subset_comparison.iloc[0]['features']  # Melhor subconjunto\n",
        "    best_exhaustive_ir = subset_comparison.iloc[0]['inconsistency_rate']  # Melhor IR\n",
        "\n",
        "    print(f\"\\nMelhor subconjunto encontrado:\")\n",
        "    print(f\"  Número de atributos: {len(best_exhaustive_subset)}\")\n",
        "    print(f\"  Taxa de inconsistência: {best_exhaustive_ir:.4f}\")\n",
        "    print(f\"  Melhoria: {((initial_ir - best_exhaustive_ir) / initial_ir * 100):.2f}%\")\n",
        "\n",
        "    # Inicializa e executa Random Restart Hill Climbing\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FASE 2: OTIMIZAÇÃO RRHC\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Iniciando otimização Random Restart Hill Climbing...\")\n",
        "\n",
        "    # Cria instância do otimizador com parâmetros\n",
        "    optimizer = RandomRestartHillClimbing(\n",
        "        consistency_measure=cm,  # Passa medida de consistência\n",
        "        max_restarts=10,  # 10 reinícios aleatórios\n",
        "        max_iterations=50,  # Máximo 50 iterações por restart\n",
        "        min_features=1  # Mínimo 1 atributo\n",
        "    )\n",
        "\n",
        "    optimization_start = time.time()  # Marca tempo de início\n",
        "    result = optimizer.optimize(verbose=True)  # Executa otimização com logs\n",
        "    optimization_time = time.time() - optimization_start  # Calcula tempo total\n",
        "\n",
        "    # Exibe resultados da otimização\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"RESULTADOS DA OTIMIZAÇÃO\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Melhor subconjunto encontrado: {len(result['best_subset'])} atributos\")\n",
        "    print(f\"Atributos selecionados: {result['best_subset']}\")\n",
        "    print(f\"Taxa de inconsistência otimizada: {result['best_ir']:.4f}\")\n",
        "\n",
        "    # Compara métodos e seleciona melhor solução\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"COMPARAÇÃO DOS MÉTODOS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Avaliação inteligente:\")\n",
        "    print(f\"  Melhor subconjunto: {len(best_exhaustive_subset)} atributos\")\n",
        "    print(f\"  Taxa de inconsistência: {best_exhaustive_ir:.4f}\")\n",
        "    print(f\"\\nRandom Restart Hill Climbing:\")\n",
        "    print(f\"  Melhor subconjunto: {len(result['best_subset'])} atributos\")\n",
        "    print(f\"  Taxa de inconsistência: {result['best_ir']:.4f}\")\n",
        "    print(f\"  Avaliações totais: {result['total_evaluations']}\")\n",
        "    print(f\"  Tempo de otimização: {optimization_time:.2f}s\")\n",
        "\n",
        "    # Seleciona melhor resultado entre os dois métodos\n",
        "    if result['best_ir'] <= best_exhaustive_ir:  # Se RRHC foi melhor ou igual\n",
        "        print(f\"✓ RRHC encontrou solução igual ou melhor!\")\n",
        "        final_best_subset = result['best_subset']  # Usa resultado do RRHC\n",
        "        final_best_ir = result['best_ir']\n",
        "    else:  # Se avaliação inteligente foi melhor\n",
        "        print(f\"✓ Avaliação inteligente encontrou melhor solução!\")\n",
        "        # Calcula quanto RRHC poderia melhorar\n",
        "        improvement_possible = ((result['best_ir'] - best_exhaustive_ir) / result['best_ir'] * 100)\n",
        "        print(f\"  Melhoria possível de RRHC: {improvement_possible:.2f}%\")\n",
        "        final_best_subset = best_exhaustive_subset  # Usa resultado da avaliação\n",
        "        final_best_ir = best_exhaustive_ir\n",
        "\n",
        "    # Imprime solução final escolhida\n",
        "    print(f\"\\nSOLUÇÃO FINAL:\")\n",
        "    print(f\"  Número de atributos selecionados: {len(final_best_subset)}\")\n",
        "    print(f\"  Taxa de inconsistência final: {final_best_ir:.4f}\")\n",
        "    # Calcula melhoria em relação ao inicial\n",
        "    print(f\"  Melhoria vs inicial: {((initial_ir - final_best_ir) / initial_ir * 100):.2f}%\")\n",
        "    # Calcula redução percentual de atributos\n",
        "    reduction = ((len(cm.feature_columns) - len(final_best_subset)) / len(cm.feature_columns) * 100)\n",
        "    print(f\"  Redução de atributos: {reduction:.1f}%\")\n",
        "\n",
        "    # Cria dataset otimizado com o melhor subconjunto\n",
        "    selected_columns = final_best_subset + [class_column]  # Adiciona coluna de classe\n",
        "    optimized_data = data[selected_columns]  # Seleciona apenas colunas escolhidas\n",
        "\n",
        "    # Salva dataset otimizado\n",
        "    print(f\"\\nSalvando dataset otimizado...\")\n",
        "    save_to_arff(optimized_data, output_file_path, \"optimized_data_RRHC\")  # Salva em ARFF\n",
        "    print(f\"Dataset salvo: {output_file_path}\")\n",
        "\n",
        "    # Análise adicional detalhada\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ANÁLISE DETALHADA\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Análise de padrões para subconjunto otimizado\n",
        "    print(f\"\\nAnálise de padrões para o subconjunto otimizado:\")\n",
        "    pattern_analysis = cm.analyze_pattern_details(final_best_subset)  # Analisa padrões\n",
        "    print(f\"  Padrões totais: {pattern_analysis['total_patterns']}\")\n",
        "    print(f\"  Padrões inconsistentes: {pattern_analysis['inconsistent_patterns']}\")\n",
        "    # Score de consistência (quanto maior, melhor)\n",
        "    print(f\"  Score de consistência: {(1 - pattern_analysis['inconsistency_rate']):.4f}\")\n",
        "\n",
        "    # Mostra alguns padrões inconsistentes se existirem\n",
        "    if pattern_analysis['inconsistent_patterns'] > 0:\n",
        "        print(f\"\\nExemplos de padrões inconsistentes (primeiros 3):\")\n",
        "        # Filtra apenas padrões inconsistentes e pega os 3 primeiros\n",
        "        inconsistent_patterns = [detail for detail in pattern_analysis['pattern_details']\n",
        "                               if detail['is_inconsistent']][:3]\n",
        "        for i, detail in enumerate(inconsistent_patterns):  # Para cada padrão\n",
        "            # Imprime informações do padrão\n",
        "            print(f\"  {i+1}. Padrão {detail['pattern']}: {detail['total_instances']} instâncias\")\n",
        "            print(f\"     Distribuição de classes: {detail['class_distribution']}\")\n",
        "            print(f\"     Número de inconsistências: {detail['inconsistency_count']}\")\n",
        "\n",
        "    # Mostra atributos selecionados\n",
        "    print(f\"\\nAtributos selecionados no melhor subconjunto:\")\n",
        "    for i, feature in enumerate(final_best_subset, 1):  # Enumera começando de 1\n",
        "        print(f\"  {i:2d}. {feature}\")  # Imprime número e nome do atributo\n",
        "\n",
        "    # Análise por tamanho de subconjunto\n",
        "    if len(subset_comparison) > 10:  # Se há dados suficientes\n",
        "        print(f\"\\nAnálise por tamanho de subconjunto:\")\n",
        "        size_analysis = {}  # Dicionário para agrupar por tamanho\n",
        "\n",
        "        # Agrupa IRs por tamanho de subconjunto\n",
        "        for _, row in subset_comparison.iterrows():\n",
        "            size = row['num_features']  # Tamanho do subconjunto\n",
        "            if size not in size_analysis:  # Se tamanho ainda não está no dicionário\n",
        "                size_analysis[size] = []  # Cria lista vazia\n",
        "            size_analysis[size].append(row['inconsistency_rate'])  # Adiciona IR à lista\n",
        "\n",
        "        # Mostra estatísticas para cada tamanho\n",
        "        for size in sorted(size_analysis.keys())[:10]:  # Primeiros 10 tamanhos\n",
        "            rates = size_analysis[size]  # Lista de IRs para este tamanho\n",
        "            avg_rate = np.mean(rates)  # Calcula média\n",
        "            min_rate = min(rates)  # Encontra mínimo\n",
        "            # Imprime estatísticas\n",
        "            print(f\"  Tamanho {size:2d}: IR médio = {avg_rate:.4f}, IR mínimo = {min_rate:.4f} \"\n",
        "                  f\"({len(rates)} subconjuntos)\")\n",
        "\n",
        "    # Retorna dicionário com todos os resultados para uso posterior\n",
        "    return {\n",
        "        'optimized_data': optimized_data,  # DataFrame otimizado\n",
        "        'best_subset': final_best_subset,  # Melhor subconjunto de atributos\n",
        "        'best_ir': final_best_ir,  # Melhor IR encontrado\n",
        "        'initial_ir': initial_ir,  # IR inicial (referência)\n",
        "        'improvement': ((initial_ir - final_best_ir) / initial_ir * 100),  # Melhoria %\n",
        "        'subset_comparison': subset_comparison,  # DataFrame com comparações\n",
        "        'rrhc_result': result  # Resultados detalhados do RRHC\n",
        "    }\n",
        "\n",
        "# Ponto de entrada do programa\n",
        "if __name__ == \"__main__\":\n",
        "    # Executa a função principal quando script é executado diretamente\n",
        "    main()"
      ]
    }
  ]
}