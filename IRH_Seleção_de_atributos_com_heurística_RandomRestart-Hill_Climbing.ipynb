{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgk22to7RwWH79VgiTx76M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Feranie/Hierarchical-Classification-Project/blob/main/IRH_Sele%C3%A7%C3%A3o_de_atributos_com_heur%C3%ADstica_RandomRestart-Hill_Climbing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XmuaihRm1a2c",
        "outputId": "d3ed1b0e-1f92-412c-c7c5-e8878e15998b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando dados...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/EC-PrintsTRA0.arff'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1566310723.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;31m# Ponto de entrada do programa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Executa a função principal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1566310723.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Carregando dados...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_arff_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file_path\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Lê o arquivo ARFF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m     \u001b[0mload_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1566310723.py\u001b[0m in \u001b[0;36mread_arff_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Abre o arquivo ARFF com codificação UTF-8 para suportar caracteres especiais\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Itera sobre cada linha do arquivo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/EC-PrintsTRA0.arff'"
          ]
        }
      ],
      "source": [
        "# Importações necessárias para o funcionamento do algoritmo\n",
        "import pandas as pd  # Biblioteca para manipulação de dados em formato DataFrame\n",
        "import numpy as np   # Biblioteca para cálculos numéricos eficientes\n",
        "import random       # Módulo para geração de números aleatórios\n",
        "import re          # Módulo para expressões regulares (usado na leitura do ARFF)\n",
        "from collections import defaultdict  # Dicionário com valores padrão\n",
        "from typing import List, Set, Tuple  # Tipos para anotações de tipo\n",
        "import time        # Módulo para medição de tempo de execução\n",
        "from concurrent.futures import ProcessPoolExecutor  # Para processamento paralelo\n",
        "import multiprocessing as mp  # Módulo para multiprocessamento\n",
        "\n",
        "# --- SEÇÃO: Leitura de arquivos ARFF ---\n",
        "def read_arff_file(file_path):\n",
        "    \"\"\"\n",
        "    Função para ler arquivos no formato ARFF (Attribute-Relation File Format)\n",
        "    usado pelo Weka e outras ferramentas de mineração de dados\n",
        "    \"\"\"\n",
        "    data = []        # Lista para armazenar os dados de cada instância\n",
        "    attributes = []  # Lista para armazenar os nomes dos atributos\n",
        "    current_section = None  # Variável para controlar qual seção do arquivo estamos lendo\n",
        "\n",
        "    # Abre o arquivo ARFF com codificação UTF-8 para suportar caracteres especiais\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        # Itera sobre cada linha do arquivo\n",
        "        for line in file:\n",
        "            line = line.strip()  # Remove espaços em branco no início e fim da linha\n",
        "            # Pula linhas vazias ou que começam com '%' (comentários no ARFF)\n",
        "            if not line or line.startswith('%'):\n",
        "                continue\n",
        "\n",
        "            # Identifica a seção @relation (nome da relação/dataset)\n",
        "            if '@relation' in line.lower():\n",
        "                current_section = 'relation'\n",
        "            # Identifica a seção @attribute (definição dos atributos)\n",
        "            elif '@attribute' in line.lower():\n",
        "                current_section = 'attribute'\n",
        "                # Usa expressão regular para extrair o nome do atributo\n",
        "                match = re.match(r'@attribute\\s+([^\\s]+)\\s+.*', line, re.IGNORECASE)\n",
        "                if match:\n",
        "                    # Adiciona o nome do atributo à lista de atributos\n",
        "                    attributes.append(match.group(1))\n",
        "            # Identifica a seção @data (início dos dados)\n",
        "            elif '@data' in line.lower():\n",
        "                current_section = 'data'\n",
        "            # Se estamos na seção de dados, processa cada linha como uma instância\n",
        "            elif current_section == 'data':\n",
        "                # Separa os valores por vírgula (considerando possíveis vírgulas em strings)\n",
        "                values = re.findall(r'[^,]+(?:,(?=[^,]*$))?', line)\n",
        "                # Remove aspas e espaços dos valores\n",
        "                values = [v.strip('\" ') for v in values]\n",
        "                # Verifica se o número de valores corresponde ao número de atributos\n",
        "                if len(values) == len(attributes):\n",
        "                    data.append(values)  # Adiciona a instância à lista de dados\n",
        "\n",
        "    # Retorna um DataFrame do pandas com os dados e nomes das colunas\n",
        "    return pd.DataFrame(data, columns=attributes)\n",
        "\n",
        "# --- SEÇÃO: Salvamento de arquivos ARFF ---\n",
        "def save_to_arff(df, file_path, relation_name=\"filtered_data\"):\n",
        "    \"\"\"\n",
        "    Função para salvar um DataFrame em formato ARFF\n",
        "    \"\"\"\n",
        "    # Abre o arquivo para escrita com codificação UTF-8\n",
        "    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "        # Escreve o cabeçalho com o nome da relação\n",
        "        f.write(f\"@relation {relation_name}\\n\\n\")\n",
        "\n",
        "        # Para cada coluna do DataFrame, define o tipo do atributo\n",
        "        for column in df.columns:\n",
        "            unique_values = df[column].unique()  # Obtém valores únicos da coluna\n",
        "            # Se todos os valores são strings, cria um atributo categórico\n",
        "            if all(isinstance(val, str) for val in unique_values):\n",
        "                vals = ','.join(sorted(set(unique_values)))  # Junta os valores únicos\n",
        "                f.write(f\"@attribute {column} {{{vals}}}\\n\")\n",
        "            else:\n",
        "                # Caso contrário, define como atributo numérico\n",
        "                f.write(f\"@attribute {column} numeric\\n\")\n",
        "\n",
        "        # Escreve o marcador de início dos dados\n",
        "        f.write(\"\\n@data\\n\")\n",
        "        # Escreve cada linha do DataFrame como uma instância\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(\",\".join(map(str, row)) + \"\\n\")\n",
        "\n",
        "# --- SEÇÃO: Classe principal para cálculo do IRH (Taxa de Inconsistência Hierárquica) ---\n",
        "class IRHCalculator:\n",
        "    \"\"\"\n",
        "    Calculadora otimizada para o IRH (Inconsistency Rate Hierarchical)\n",
        "    usado em problemas de classificação hierárquica\n",
        "    \"\"\"\n",
        "    def __init__(self, data: pd.DataFrame, weight_mode='equal'):\n",
        "        \"\"\"\n",
        "        Inicializa o calculador de IRH\n",
        "        \"\"\"\n",
        "        self.data = data  # Armazena o DataFrame com os dados\n",
        "        self.weight_mode = weight_mode  # Modo de cálculo dos pesos ('equal', 'softmax', 'hybrid')\n",
        "        self.h = self._get_number_of_levels()  # Calcula o número de níveis na hierarquia\n",
        "        self.weights = self._calculate_weights()  # Calcula os pesos para cada nível\n",
        "        # Cache para evitar recalcular patterns já processados (otimização)\n",
        "        self._pattern_cache = {}\n",
        "        # Pré-calcula os patterns de classe para cada nível (otimização)\n",
        "        self._precompute_class_patterns()\n",
        "\n",
        "    def _get_number_of_levels(self) -> int:\n",
        "        \"\"\"\n",
        "        Determina o número máximo de níveis na hierarquia de classes\n",
        "        \"\"\"\n",
        "        # Conta o número de pontos em cada classe e pega o máximo\n",
        "        return max(len(str(c).split('.')) for c in self.data['class'])\n",
        "\n",
        "    def _precompute_class_patterns(self):\n",
        "        \"\"\"\n",
        "        Pré-calcula os patterns de classe para cada nível hierárquico\n",
        "        Esta otimização evita recalcular os patterns a cada chamada\n",
        "        \"\"\"\n",
        "        self.class_patterns_by_level = {}  # Dicionário para armazenar patterns por nível\n",
        "        # Para cada nível da hierarquia\n",
        "        for level in range(1, self.h + 1):\n",
        "            class_patterns = []  # Lista para armazenar patterns deste nível\n",
        "            # Para cada valor de classe nos dados\n",
        "            for class_val in self.data['class']:\n",
        "                class_parts = str(class_val).split('.')  # Separa por pontos\n",
        "                # Se há partes suficientes para este nível\n",
        "                if len(class_parts) >= level:\n",
        "                    # Pega apenas as primeiras 'level' partes\n",
        "                    class_patterns.append('.'.join(class_parts[:level]))\n",
        "                else:\n",
        "                    # Se não há partes suficientes, usa a classe completa\n",
        "                    class_patterns.append(str(class_val))\n",
        "            # Armazena os patterns deste nível\n",
        "            self.class_patterns_by_level[level] = class_patterns\n",
        "\n",
        "    def _calculate_weights(self):\n",
        "        \"\"\"\n",
        "        Calcula os pesos para cada nível da hierarquia\n",
        "        \"\"\"\n",
        "        if self.weight_mode == 'equal':\n",
        "            # Pesos iguais para todos os níveis\n",
        "            return [1/self.h for _ in range(self.h)]\n",
        "\n",
        "        # Para outros modos, retorna None (será calculado dinamicamente)\n",
        "        return None\n",
        "\n",
        "    def _calculate_level_inconsistency_rate_optimized(self, attribute_subset: List[str], level: int) -> float:\n",
        "        \"\"\"\n",
        "        Versão otimizada do cálculo da taxa de inconsistência para um nível específico\n",
        "        \"\"\"\n",
        "        # Cria uma chave única para o cache baseada nos atributos e nível\n",
        "        cache_key = (tuple(sorted(attribute_subset)), level)\n",
        "        # Se já calculamos este resultado antes, retorna do cache\n",
        "        if cache_key in self._pattern_cache:\n",
        "            return self._pattern_cache[cache_key]\n",
        "\n",
        "        # Obtém apenas os dados dos atributos selecionados como array numpy (mais rápido)\n",
        "        subset_data = self.data[attribute_subset].values\n",
        "        # Obtém os rótulos de classe para este nível específico\n",
        "        class_labels = self.class_patterns_by_level[level]\n",
        "\n",
        "        # Dicionário aninhado: pattern -> {classe -> contagem}\n",
        "        pattern_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "        # Para cada instância nos dados\n",
        "        for i, pattern in enumerate(subset_data):\n",
        "            pattern_tuple = tuple(pattern)  # Converte para tupla (imutável, pode ser chave)\n",
        "            # Incrementa a contagem desta combinação pattern-classe\n",
        "            pattern_counts[pattern_tuple][class_labels[i]] += 1\n",
        "\n",
        "        inconsistency_count = 0  # Contador de instâncias inconsistentes\n",
        "        # Para cada pattern único encontrado\n",
        "        for class_counts in pattern_counts.values():\n",
        "            total_pattern_count = sum(class_counts.values())  # Total de instâncias com este pattern\n",
        "            max_class_count = max(class_counts.values())      # Maior contagem de uma classe neste pattern\n",
        "            # Instâncias inconsistentes = total - maior classe (instâncias que não pertencem à classe majoritária)\n",
        "            inconsistency_count += total_pattern_count - max_class_count\n",
        "\n",
        "        # Calcula a taxa de inconsistência (proporção de instâncias inconsistentes)\n",
        "        result = inconsistency_count / len(self.data) if len(self.data) > 0 else float('inf')\n",
        "        # Armazena no cache para uso futuro\n",
        "        self._pattern_cache[cache_key] = result\n",
        "        return result\n",
        "\n",
        "    def calculate_irh(self, attribute_subset: List[str]) -> float:\n",
        "        \"\"\"\n",
        "        Calcula o IRH (Taxa de Inconsistência Hierárquica) para um subconjunto de atributos\n",
        "        \"\"\"\n",
        "        # Se não há atributos, retorna infinito (pior caso possível)\n",
        "        if not attribute_subset:\n",
        "            return float('inf')\n",
        "\n",
        "        # Calcula a taxa de inconsistência para cada nível da hierarquia\n",
        "        ir_levels = [\n",
        "            self._calculate_level_inconsistency_rate_optimized(attribute_subset, level)\n",
        "            for level in range(1, self.h + 1)\n",
        "        ]\n",
        "\n",
        "        # Calcula os pesos baseado no modo selecionado\n",
        "        if self.weight_mode == 'equal':\n",
        "            weights = self.weights  # Usa pesos pré-calculados (iguais)\n",
        "        elif self.weight_mode == 'softmax':\n",
        "            weights = self._softmax(ir_levels, T=2.0)  # Pesos baseados em softmax\n",
        "        elif self.weight_mode == 'hybrid':\n",
        "            # Combina pesos iguais com softmax\n",
        "            equal = [1/self.h for _ in range(self.h)]\n",
        "            soft = self._softmax(ir_levels, T=2.0)\n",
        "            alpha = 0.5  # Fator de combinação\n",
        "            weights = [(1 - alpha) * e + alpha * s for e, s in zip(equal, soft)]\n",
        "        else:\n",
        "            raise ValueError(\"weight_mode deve ser 'equal', 'softmax' ou 'hybrid'\")\n",
        "\n",
        "        # Retorna a soma ponderada das taxas de inconsistência de todos os níveis\n",
        "        return sum(w * ir for w, ir in zip(weights, ir_levels))\n",
        "\n",
        "    def _softmax(self, x: List[float], T: float = 2.0) -> List[float]:\n",
        "        \"\"\"\n",
        "        Calcula softmax dos valores para gerar pesos adaptativos\n",
        "        T é a temperatura que controla a suavidade da distribuição\n",
        "        \"\"\"\n",
        "        x = np.array(x)  # Converte para array numpy\n",
        "        e_x = np.exp(-x / T)  # Exponencial negativa (valores menores de IR têm peso maior)\n",
        "        return (e_x / e_x.sum()).tolist()  # Normaliza para somar 1\n",
        "\n",
        "# --- SEÇÃO: Classe principal do algoritmo Hill Climbing ---\n",
        "class RandomRestartHillClimbingFeatureSelector:\n",
        "    \"\"\"\n",
        "    Implementação otimizada do algoritmo Random Restart Hill Climbing\n",
        "    para seleção de características em problemas de classificação hierárquica\n",
        "    \"\"\"\n",
        "    def __init__(self, data, weight_mode='equal', max_iterations=100, restarts=10,\n",
        "                 early_stopping_patience=20, min_improvement=1e-6):\n",
        "        \"\"\"\n",
        "        Inicializa o seletor de características\n",
        "        \"\"\"\n",
        "        self.data = data  # Dataset completo\n",
        "        # Lista de características (todas as colunas exceto 'class')\n",
        "        self.features = [col for col in data.columns if col != 'class']\n",
        "        # Inicializa o calculador de IRH\n",
        "        self.irh_calculator = IRHCalculator(data, weight_mode)\n",
        "        self.max_iterations = max_iterations  # Número máximo de iterações por restart\n",
        "        self.restarts = restarts  # Número de reinicializações aleatórias\n",
        "        self.early_stopping_patience = early_stopping_patience  # Paciência para parada precoce\n",
        "        self.min_improvement = min_improvement  # Melhoria mínima considerada significativa\n",
        "        random.seed(42)  # Fixa a semente para reprodutibilidade\n",
        "\n",
        "    def fitness(self, feature_subset):\n",
        "        \"\"\"\n",
        "        Função de fitness - quanto menor o IRH, melhor o subconjunto\n",
        "        \"\"\"\n",
        "        return self.irh_calculator.calculate_irh(feature_subset)\n",
        "\n",
        "    def get_neighbors_incremental(self, current_subset: List[str]) -> List[Tuple[List[str], str]]:\n",
        "        \"\"\"\n",
        "        Gera todos os vizinhos possíveis do subconjunto atual\n",
        "        Retorna tuplas (vizinho, tipo_de_ação) para tracking\n",
        "        \"\"\"\n",
        "        neighbors = []  # Lista para armazenar vizinhos\n",
        "        current_set = set(current_subset)  # Converte para set para busca eficiente\n",
        "\n",
        "        # OPERAÇÃO: Adicionar um atributo\n",
        "        # Prioriza adições pois frequentemente levam a melhorias\n",
        "        for f in self.features:\n",
        "            if f not in current_set:  # Se o atributo não está no subconjunto atual\n",
        "                # Cria vizinho adicionando este atributo\n",
        "                neighbors.append((current_subset + [f], 'add'))\n",
        "\n",
        "        # OPERAÇÃO: Remover um atributo\n",
        "        # Só permite remoção se sobrar pelo menos 1 atributo\n",
        "        if len(current_subset) > 1:\n",
        "            for f in current_subset:\n",
        "                # Cria vizinho removendo este atributo\n",
        "                neighbors.append(([x for x in current_subset if x != f], 'remove'))\n",
        "\n",
        "        return neighbors\n",
        "\n",
        "    def hill_climb_optimized(self, initial_subset):\n",
        "        \"\"\"\n",
        "        Implementação otimizada do algoritmo Hill Climbing\n",
        "        \"\"\"\n",
        "        current_subset = initial_subset  # Subconjunto atual\n",
        "        current_score = self.fitness(current_subset)  # Score atual\n",
        "        no_improvement_count = 0  # Contador para parada precoce\n",
        "\n",
        "        print(f\"  Início hill climb com {len(initial_subset)} atributos, IRH inicial: {current_score:.6f}\")\n",
        "\n",
        "        # Loop principal do Hill Climbing\n",
        "        for iteration in range(self.max_iterations):\n",
        "            # Gera todos os vizinhos possíveis\n",
        "            neighbors = self.get_neighbors_incremental(current_subset)\n",
        "            best_neighbor = current_subset  # Melhor vizinho encontrado\n",
        "            best_score = current_score      # Melhor score encontrado\n",
        "            best_action = None             # Ação que levou ao melhor vizinho\n",
        "\n",
        "            # Lista para armazenar melhorias significativas\n",
        "            improvements = []\n",
        "            # Avalia cada vizinho\n",
        "            for neighbor, action in neighbors:\n",
        "                score = self.fitness(neighbor)  # Calcula IRH do vizinho\n",
        "                # Se há melhoria significativa (redução no IRH)\n",
        "                if score < current_score - self.min_improvement:\n",
        "                    improvements.append((neighbor, score, action))\n",
        "\n",
        "            # Se encontrou melhorias\n",
        "            if improvements:\n",
        "                # Seleciona a melhor melhoria (menor IRH)\n",
        "                best_neighbor, best_score, best_action = min(improvements, key=lambda x: x[1])\n",
        "                current_subset = best_neighbor  # Atualiza subconjunto atual\n",
        "                current_score = best_score      # Atualiza score atual\n",
        "                no_improvement_count = 0        # Reseta contador de parada precoce\n",
        "\n",
        "                # Log periódico do progresso\n",
        "                if iteration % 10 == 0 or best_action:\n",
        "                    print(f\"    Iteração {iteration}: {best_action} -> IRH={current_score:.6f}, tamanho={len(current_subset)}\")\n",
        "            else:\n",
        "                # Não houve melhoria\n",
        "                no_improvement_count += 1\n",
        "                # Se excedeu a paciência, para o algoritmo\n",
        "                if no_improvement_count >= self.early_stopping_patience:\n",
        "                    print(f\"    Parada precoce após {iteration} iterações (sem melhoria)\")\n",
        "                    break\n",
        "\n",
        "        return current_subset, current_score\n",
        "\n",
        "    def run_parallel(self):\n",
        "        \"\"\"\n",
        "        Executa múltiplos restarts do Hill Climbing (versão paralelizada)\n",
        "        \"\"\"\n",
        "        print(f\"Iniciando RRHC otimizado com {self.restarts} restarts...\")\n",
        "\n",
        "        best_global_subset = None   # Melhor subconjunto global\n",
        "        best_global_score = float('inf')  # Melhor score global\n",
        "\n",
        "        # Executa cada restart sequencialmente\n",
        "        for restart in range(self.restarts):\n",
        "            print(f\"\\n=== Restart {restart + 1}/{self.restarts} ===\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Gera ponto de partida inteligente\n",
        "            # Tamanho inicial entre 10% e 25% das características\n",
        "            initial_size = random.randint(max(1, len(self.features) // 10),\n",
        "                                        min(len(self.features), len(self.features) // 4))\n",
        "            # Seleciona características aleatoriamente para o ponto inicial\n",
        "            initial_feature = random.sample(self.features, initial_size)\n",
        "\n",
        "            # Executa Hill Climbing a partir deste ponto inicial\n",
        "            subset, score = self.hill_climb_optimized(initial_feature)\n",
        "            elapsed = time.time() - start_time\n",
        "\n",
        "            print(f\"Restart {restart + 1} finalizado em {elapsed:.2f}s — IRH: {score:.6f}, tamanho: {len(subset)}\")\n",
        "\n",
        "            # Se encontrou uma solução melhor que a atual melhor\n",
        "            if score < best_global_score:\n",
        "                best_global_subset = subset\n",
        "                best_global_score = score\n",
        "                print(f\"*** NOVA MELHOR SOLUÇÃO: {best_global_score:.6f} ***\")\n",
        "\n",
        "        return best_global_subset, best_global_score\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Método principal para executar a seleção de características\n",
        "        \"\"\"\n",
        "        return self.run_parallel()\n",
        "\n",
        "# --- SEÇÃO: Funções auxiliares para paralelização ---\n",
        "def evaluate_restart(args):\n",
        "    \"\"\"\n",
        "    Função auxiliar para avaliar um restart em paralelo\n",
        "    (usado na versão totalmente paralelizada)\n",
        "    \"\"\"\n",
        "    # Desempacota os argumentos\n",
        "    data, features, weight_mode, max_iterations, restart_id, seed = args\n",
        "\n",
        "    # Fixa semente única para este restart\n",
        "    random.seed(seed + restart_id)\n",
        "\n",
        "    # Cria seletor para este restart\n",
        "    selector = OptimizedRandomRestartHillClimbingFeatureSelector(\n",
        "        data, weight_mode=weight_mode, max_iterations=max_iterations, restarts=1\n",
        "    )\n",
        "\n",
        "    # Gera ponto inicial aleatório\n",
        "    initial_size = random.randint(max(1, len(features) // 10),\n",
        "                                min(len(features), len(features) // 4))\n",
        "    initial_feature = random.sample(features, initial_size)\n",
        "\n",
        "    # Executa Hill Climbing\n",
        "    subset, score = selector.hill_climb_optimized(initial_feature)\n",
        "\n",
        "    return subset, score, restart_id\n",
        "\n",
        "# --- SEÇÃO: Versão com paralelização completa ---\n",
        "class ParallelRandomRestartHillClimbing:\n",
        "    \"\"\"\n",
        "    Versão completamente paralelizada do Random Restart Hill Climbing\n",
        "    \"\"\"\n",
        "    def __init__(self, data, weight_mode='equal', max_iterations=100, restarts=10):\n",
        "        \"\"\"\n",
        "        Inicializa o seletor paralelizado\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.features = [col for col in data.columns if col != 'class']\n",
        "        self.weight_mode = weight_mode\n",
        "        self.max_iterations = max_iterations\n",
        "        self.restarts = restarts\n",
        "\n",
        "    def run_parallel_full(self):\n",
        "        \"\"\"\n",
        "        Executa todos os restarts em paralelo usando múltiplos processos\n",
        "        \"\"\"\n",
        "        print(f\"Iniciando RRHC paralelizado com {self.restarts} restarts em {mp.cpu_count()} processadores...\")\n",
        "\n",
        "        # Prepara argumentos para cada restart\n",
        "        args_list = [\n",
        "            (self.data, self.features, self.weight_mode, self.max_iterations, i, 42)\n",
        "            for i in range(self.restarts)\n",
        "        ]\n",
        "\n",
        "        best_global_subset = None\n",
        "        best_global_score = float('inf')\n",
        "\n",
        "        # Executa restarts em paralelo usando ProcessPoolExecutor\n",
        "        with ProcessPoolExecutor(max_workers=min(mp.cpu_count(), self.restarts)) as executor:\n",
        "            # Mapeia a função evaluate_restart para cada conjunto de argumentos\n",
        "            results = list(executor.map(evaluate_restart, args_list))\n",
        "\n",
        "        # Analisa os resultados de todos os restarts\n",
        "        for subset, score, restart_id in results:\n",
        "            print(f\"Restart {restart_id + 1}: IRH={score:.6f}, tamanho={len(subset)}\")\n",
        "            # Se encontrou melhor solução\n",
        "            if score < best_global_score:\n",
        "                best_global_subset = subset\n",
        "                best_global_score = score\n",
        "\n",
        "        return best_global_subset, best_global_score\n",
        "\n",
        "# --- SEÇÃO: Função principal do programa ---\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Função principal que orquestra todo o processo de seleção de características\n",
        "    \"\"\"\n",
        "    # Define caminhos dos arquivos de entrada e saída\n",
        "    input_file_path = '/content/EC-PrintsTRA0.arff'  # Arquivo ARFF de entrada\n",
        "    output_file_path = '/content/EC-PrintsTRA0ptimizedRRHC.arff'  # Arquivo ARFF de saída\n",
        "\n",
        "    # ETAPA 1: Carregamento dos dados\n",
        "    print(\"Carregando dados...\")\n",
        "    start_time = time.time()\n",
        "    data = read_arff_file(input_file_path)  # Lê o arquivo ARFF\n",
        "    load_time = time.time() - start_time\n",
        "\n",
        "    print(f\"Carregados {len(data)} instâncias com {len(data.columns)-1} atributos em {load_time:.2f}s\")\n",
        "\n",
        "    # ETAPA 2: Configuração e execução do algoritmo\n",
        "    print(\"\\n=== VERSÃO OTIMIZADA SEQUENCIAL ===\")\n",
        "    selector = RandomRestartHillClimbingFeatureSelector(\n",
        "        data,\n",
        "        weight_mode='hybrid',       # Modo híbrido para cálculo de pesos\n",
        "        max_iterations=50,          # Máximo de 50 iterações por restart\n",
        "        restarts=10,               # 10 reinicializações aleatórias\n",
        "        early_stopping_patience=15, # Para após 15 iterações sem melhoria\n",
        "        min_improvement=1e-6       # Melhoria mínima de 0.000001\n",
        "    )\n",
        "\n",
        "    # Executa o algoritmo e mede o tempo\n",
        "    start_time = time.time()\n",
        "    best_subset, best_irh = selector.run()\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    # ETAPA 3: Apresentação dos resultados\n",
        "    print(f\"\\n=== RESULTADOS FINAIS ===\")\n",
        "    print(f\"Tempo total: {total_time:.2f}s\")\n",
        "    print(f\"Melhores atributos ({len(best_subset)}): {best_subset}\")\n",
        "    print(f\"IRH obtido: {best_irh:.6f}\")\n",
        "\n",
        "    # ETAPA 4: Salvamento dos resultados\n",
        "    # Cria novo dataset apenas com os atributos selecionados + classe\n",
        "    selected_data = data[best_subset + ['class']]\n",
        "    # Salva no formato ARFF\n",
        "    save_to_arff(selected_data, output_file_path, relation_name=\"Optimized_Data\")\n",
        "    print(f\"Resultados salvos em: {output_file_path}\")\n",
        "\n",
        "# Ponto de entrada do programa\n",
        "if __name__ == \"__main__\":\n",
        "    main()  # Executa a função principal"
      ]
    }
  ]
}