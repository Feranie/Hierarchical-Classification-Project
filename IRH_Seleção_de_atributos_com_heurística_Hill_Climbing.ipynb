{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKTZiB9T9WwMcsJmqzn+TN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Feranie/Hierarchical-Classification-Project/blob/main/IRH_Sele%C3%A7%C3%A3o_de_atributos_com_heur%C3%ADstica_Hill_Climbing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmuaihRm1a2c"
      },
      "outputs": [],
      "source": [
        "# Importando bibliotecas necessárias\n",
        "import pandas as pd  # Biblioteca para manipulação de dados em formato de tabela\n",
        "import random  # Biblioteca para geração de números aleatórios\n",
        "from collections import defaultdict  # Dicionário com valor padrão para chaves inexistentes\n",
        "from typing import List, Set, Tuple  # Tipos para anotações de tipo\n",
        "\n",
        "# Classe para leitura de arquivos ARFF (formato usado pelo software WEKA para mineração de dados)\n",
        "class ArffReader:\n",
        "    def __init__(self):\n",
        "        self.relation = \"\"  # Armazena o nome da relação/conjunto de dados\n",
        "        self.attributes = []  # Lista para armazenar os atributos e seus tipos\n",
        "        self.data = []  # Lista para armazenar os dados/instâncias\n",
        "\n",
        "    # Método para ler um arquivo ARFF\n",
        "    def read_arff(self, input_file):\n",
        "        reading_data = False  # Flag para identificar quando estamos na seção de dados\n",
        "        try:\n",
        "            with open(input_file, 'r', encoding='utf-8') as file:  # Abre o arquivo com codificação UTF-8\n",
        "                for line in file:  # Itera sobre cada linha do arquivo\n",
        "                    line = line.strip()  # Remove espaços em branco no início e fim da linha\n",
        "                    if not line or line.startswith('%'):  # Ignora linhas vazias ou comentários\n",
        "                        continue\n",
        "                    if line.lower().startswith('@relation'):  # Identifica a linha que define o nome da relação\n",
        "                        self.relation = line.split(' ', 1)[1]  # Extrai o nome da relação\n",
        "                    elif line.lower().startswith('@attribute'):  # Identifica linhas que definem atributos\n",
        "                        parts = line.split(' ', 2)  # Divide a linha em até 3 partes\n",
        "                        if len(parts) < 3:  # Verifica se a definição do atributo está completa\n",
        "                            raise ValueError(f\"Definição d'attribut invalide: {line}\")  # Levanta erro se definição estiver incompleta\n",
        "                        attribute_name = parts[1]  # Nome do atributo\n",
        "                        attribute_type = parts[2]  # Tipo do atributo\n",
        "                        self.attributes.append((attribute_name, attribute_type))  # Adiciona atributo à lista\n",
        "                    elif line.lower().startswith('@data'):  # Identifica início da seção de dados\n",
        "                        reading_data = True  # Ativa a flag de leitura de dados\n",
        "                    elif reading_data:  # Se estiver na seção de dados\n",
        "                        if ',' in line:  # Verifica se a linha contém dados separados por vírgula\n",
        "                            values = line.split(',')  # Divide os valores\n",
        "                            if len(values) == len(self.attributes):  # Verifica se o número de valores corresponde ao número de atributos\n",
        "                                self.data.append(values)  # Adiciona os valores aos dados\n",
        "                            else:\n",
        "                                print(f\"Avertissement: Ligne ignorée en raison d'un nombre incorrect de valeurs: {line}\")  # Avisa sobre linhas ignoradas\n",
        "            # Imprime informações sobre os dados lidos\n",
        "            print(f\"Fichier lu avec succès. Total d'instances: {len(self.data)}\")\n",
        "            print(f\"Total d'attributs: {len(self.attributes)}\")\n",
        "            print(f\"Exemples de données: {self.data[:5]}\")\n",
        "        except Exception as e:  # Captura qualquer erro durante a leitura\n",
        "            print(f\"Erreur lors de la lecture du fichier: {str(e)}\")\n",
        "            raise  # Propaga o erro\n",
        "\n",
        "# Função para salvar dados em formato ARFF\n",
        "def save_to_arff(df, output_file_path, relation_name, attributes):\n",
        "    # Filtra os atributos para manter apenas os presentes no DataFrame\n",
        "    filtered_attributes = [(name, type_) for name, type_ in attributes if name in df.columns or name == 'class']\n",
        "\n",
        "    with open(output_file_path, 'w', encoding='utf-8') as f:  # Abre arquivo para escrita\n",
        "        f.write(f\"@relation {relation_name}\\n\\n\")  # Escreve o nome da relação\n",
        "        for attr_name, attr_type in filtered_attributes:  # Itera sobre os atributos filtrados\n",
        "            f.write(f\"@attribute {attr_name} {attr_type}\\n\")  # Escreve definição de cada atributo\n",
        "        f.write(\"\\n@data\\n\")  # Escreve marcador de início da seção de dados\n",
        "        for index, row in df.iterrows():  # Itera sobre as linhas do DataFrame\n",
        "            f.write(','.join(map(str, row.values)) + '\\n')  # Converte valores para string e une com vírgulas\n",
        "    print(f\"Fichier ARFF sauvegardé avec succès en {output_file_path}\")  # Confirma salvamento\n",
        "\n",
        "# Função para obter o número de níveis na classificação hierárquica\n",
        "def get_number_of_levels(data: pd.DataFrame) -> int:\n",
        "    try:\n",
        "        # Calcula o número máximo de níveis dividindo os valores da coluna 'class' por pontos\n",
        "        return max(len(str(c).split('.')) for c in data['class'])\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors du calcul du nombre de niveaux : {e}\")\n",
        "        return 0  # Retorna 0 em caso de erro\n",
        "\n",
        "# Função para calcular a taxa de inconsistência em um nível específico\n",
        "def calculate_level_inconsistency_rate(data: pd.DataFrame, attribute_subset: List[str], level: int) -> float:\n",
        "    pattern_counts = defaultdict(lambda: defaultdict(int))  # Dicionário para contar padrões de atributos\n",
        "    total_instances = len(data)  # Total de instâncias no conjunto de dados\n",
        "\n",
        "    # Conta ocorrências de cada padrão de atributos e classe no nível específico\n",
        "    for _, row in data.iterrows():  # Itera sobre cada linha do DataFrame\n",
        "        pattern = tuple(row[attr] for attr in attribute_subset)  # Cria um padrão com os valores dos atributos selecionados\n",
        "        class_parts = str(row['class']).split('.')  # Divide a classe em níveis\n",
        "        if len(class_parts) >= level:  # Verifica se a classe tem o nível necessário\n",
        "            class_label = '.'.join(class_parts[:level])  # Obtém a classe até o nível especificado\n",
        "            pattern_counts[pattern][class_label] += 1  # Incrementa a contagem para este padrão e classe\n",
        "\n",
        "    # Calcula inconsistências (quando um mesmo padrão tem diferentes classes)\n",
        "    inconsistency_count = 0\n",
        "    for pattern, class_counts in pattern_counts.items():  # Para cada padrão e suas contagens de classe\n",
        "        total_pattern_count = sum(class_counts.values())  # Total de instâncias com este padrão\n",
        "        max_class_count = max(class_counts.values())  # Contagem da classe mais frequente\n",
        "        pattern_inconsistency = total_pattern_count - max_class_count  # Instâncias inconsistentes\n",
        "        inconsistency_count += pattern_inconsistency  # Acumula inconsistências\n",
        "\n",
        "    # Retorna a taxa de inconsistência (inconsistências / total de instâncias)\n",
        "    return inconsistency_count / total_instances if total_instances > 0 else float('inf')\n",
        "\n",
        "# Função para calcular os pesos para cada nível hierárquico\n",
        "def calculate_weights(h: int) -> List[float]:\n",
        "    # Calcula pesos para cada nível, dando mais importância aos níveis superiores\n",
        "    return [(h - i + 1) * 2 / (h * (h + 1)) for i in range(1, h + 1)]\n",
        "\n",
        "# Função para calcular o IRH (Inconsistency Rate Hierarchy) - métrica para avaliar subconjuntos de atributos\n",
        "def calculate_irh(data: pd.DataFrame, attribute_subset: List[str]) -> float:\n",
        "    if not attribute_subset:  # Se o subconjunto estiver vazio\n",
        "        return float('inf')  # Retorna infinito (pior caso)\n",
        "\n",
        "    num_levels = get_number_of_levels(data)  # Obtém número de níveis hierárquicos\n",
        "    weights = calculate_weights(num_levels)  # Calcula pesos para cada nível\n",
        "\n",
        "    ir_levels = []  # Lista para armazenar taxas de inconsistência por nível\n",
        "    for level in range(1, num_levels + 1):  # Para cada nível\n",
        "        ir = calculate_level_inconsistency_rate(data, attribute_subset, level)  # Calcula taxa de inconsistência\n",
        "        ir_levels.append(ir)  # Adiciona à lista\n",
        "\n",
        "    # Retorna a soma ponderada das taxas de inconsistência\n",
        "    return sum(w * ir for w, ir in zip(weights, ir_levels))\n",
        "\n",
        "# Função para obter vizinhos de um subconjunto (adicionar ou remover um atributo)\n",
        "def get_neighbors(current_subset: Set[str], all_features: List[str]) -> List[Set[str]]:\n",
        "    neighbors = []  # Lista para armazenar subconjuntos vizinhos\n",
        "\n",
        "    # Adiciona um atributo\n",
        "    for feature in all_features:  # Para cada atributo disponível\n",
        "        if feature not in current_subset:  # Se não estiver no subconjunto atual\n",
        "            neighbor = current_subset | {feature}  # Adiciona o atributo ao subconjunto\n",
        "            neighbors.append(neighbor)  # Adiciona à lista de vizinhos\n",
        "\n",
        "    # Remove um atributo\n",
        "    for feature in current_subset:  # Para cada atributo no subconjunto atual\n",
        "        neighbor = current_subset - {feature}  # Remove o atributo do subconjunto\n",
        "        if neighbor:  # Apenas adiciona se não ficar vazio\n",
        "            neighbors.append(neighbor)  # Adiciona à lista de vizinhos\n",
        "\n",
        "    return neighbors  # Retorna todos os vizinhos\n",
        "\n",
        "# Algoritmo de seleção de atributos por subida de encosta (hill climbing)\n",
        "def hill_climbing_feature_selection(data: pd.DataFrame, max_iterations: int = 100) -> Tuple[Set[str], float]:\n",
        "    all_features = [col for col in data.columns if col != 'class']  # Lista de todos os atributos exceto a classe\n",
        "\n",
        "    current_subset = set()  # Inicia com conjunto vazio de atributos\n",
        "    current_irh = calculate_irh(data, list(current_subset))  # Calcula IRH inicial\n",
        "\n",
        "    iteration = 0  # Contador de iterações\n",
        "    while iteration < max_iterations:  # Enquanto não atingir número máximo de iterações\n",
        "        # Imprime informações da iteração atual\n",
        "        print(f\"\\nIteration {iteration + 1}\")\n",
        "        print(f\"Current subset: {current_subset}\")\n",
        "        print(f\"Current IRH: {current_irh:.4f}\")\n",
        "\n",
        "        neighbors = get_neighbors(current_subset, all_features)  # Obtém vizinhos\n",
        "        best_neighbor = None  # Melhor vizinho encontrado\n",
        "        best_neighbor_irh = float('inf')  # IRH do melhor vizinho (inicialmente infinito)\n",
        "\n",
        "        # Avalia cada vizinho\n",
        "        for neighbor in neighbors:  # Para cada vizinho\n",
        "            neighbor_irh = calculate_irh(data, list(neighbor))  # Calcula IRH do vizinho\n",
        "            print(f\"Evaluating subset {neighbor}: IRH = {neighbor_irh:.4f}\")  # Imprime avaliação\n",
        "\n",
        "            if neighbor_irh < best_neighbor_irh:  # Se encontrou vizinho melhor\n",
        "                best_neighbor = neighbor  # Atualiza melhor vizinho\n",
        "                best_neighbor_irh = neighbor_irh  # Atualiza IRH do melhor vizinho\n",
        "\n",
        "        if best_neighbor_irh >= current_irh:  # Se nenhum vizinho é melhor que o atual\n",
        "            print(\"\\nLocal optimum reached!\")  # Atingiu ótimo local\n",
        "            break  # Encerra a busca\n",
        "\n",
        "        # Atualiza para o melhor vizinho\n",
        "        current_subset = best_neighbor\n",
        "        current_irh = best_neighbor_irh\n",
        "        iteration += 1  # Incrementa contador de iterações\n",
        "\n",
        "    # Imprime resultado final\n",
        "    print(\"\\nAtributos selecionados apos the ranking :\", current_subset)\n",
        "    print(f\"IRH final : {current_irh:.4f}\")\n",
        "\n",
        "    return current_subset, current_irh  # Retorna o melhor subconjunto e seu IRH\n",
        "\n",
        "# Função principal\n",
        "def main():\n",
        "    input_file_path = '/content/GPCR-PrintsTRA6.arff'  # Caminho do arquivo de entrada\n",
        "    output_file_path = '/content/GPCR-PrintsTRA6Optimized.arff'  # Caminho do arquivo de saída\n",
        "\n",
        "    print(\"Carregamento de dados...\")  # Informa início do carregamento\n",
        "    reader = ArffReader()  # Cria instância do leitor ARFF\n",
        "    try:\n",
        "        reader.read_arff(input_file_path)  # Tenta ler o arquivo\n",
        "    except Exception as e:  # Captura erros de leitura\n",
        "        print(f\"Erreur lors de la lecture du fichier : {e}\")\n",
        "        return  # Encerra execução em caso de erro\n",
        "\n",
        "    # Converte dados lidos para DataFrame pandas\n",
        "    data = pd.DataFrame(reader.data, columns=[attr[0] for attr in reader.attributes])\n",
        "    print(f\"Total d'instances: {len(data)}\")\n",
        "    print(f\"Total d'attributs: {len(data.columns)}\")\n",
        "\n",
        "    # Executa algoritmo de seleção de atributos\n",
        "    best_subset, final_irh = hill_climbing_feature_selection(data)\n",
        "\n",
        "    # Cria DataFrame apenas com os atributos selecionados e a classe\n",
        "    selected_columns = list(best_subset) + ['class']\n",
        "    optimized_data = data[selected_columns]\n",
        "\n",
        "    # Salva os dados otimizados em formato ARFF\n",
        "    save_to_arff(optimized_data, output_file_path, reader.relation, reader.attributes)\n",
        "\n",
        "# Ponto de entrada do programa\n",
        "if __name__ == \"__main__\":\n",
        "    main()  # Executa a função principal"
      ]
    }
  ]
}